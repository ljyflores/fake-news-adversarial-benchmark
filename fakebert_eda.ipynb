{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sustainable-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn  # neural network modules\n",
    "import torch.nn.functional as F  # activation functions\n",
    "import torch.optim as optim  # optimizer\n",
    "from torch.autograd import Variable # add gradients to tensors\n",
    "from torch.nn import Parameter # model parameter functionality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aerial-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df_orig = pd.read_csv(\"FakeNewsDetection/train.csv\")\n",
    "df = df_orig.iloc[:10200].reset_index(drop=True)\n",
    "\n",
    "# Fix target label\n",
    "label_encodings = {\n",
    "    'pants-fire': 0, \n",
    "    'false':      1, \n",
    "    'barely-true':2, \n",
    "    'half-true':  3, \n",
    "    'mostly-true':4,\n",
    "    'true':       5\n",
    "}\n",
    "df['target'] = df['label'].apply(lambda x: label_encodings[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-independence",
   "metadata": {},
   "source": [
    "### Glove Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "charitable-middle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Glove file\n",
    "words = pd.read_table(\"glove.6B.100d.txt\", sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "# Generate Glove dictionary\n",
    "glove = {word:words.iloc[idx].values for (word,idx) in zip(words.index,range(words.shape[0]))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "insured-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the text into words\n",
    "df['words'] = df['statement'].apply(lambda x: x.replace('?',' ?').replace('.',' .').\\\n",
    "                                    lower().split())\n",
    "\n",
    "# Generate the list of all vocab words in our data\n",
    "target_vocab = list(itertools.chain.from_iterable(df['words']))\n",
    "target_vocab = list(set(target_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "remarkable-cologne",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We found 5373 out of 16980 words not in the Glove model\n"
     ]
    }
   ],
   "source": [
    "# Generate the weights_matrix, which is the matrix of word embeddings that we\n",
    "# pass into Pytorch. It contains len(target_vocab) rows for each of the words,\n",
    "# each represented by a length-100 vector – taken from the Glove embedding\n",
    "weights_matrix = np.zeros((len(target_vocab)+1,100))\n",
    "\n",
    "# Add in vocab\n",
    "error_count = 0\n",
    "word_to_idx = {}\n",
    "for i, word in enumerate(target_vocab):\n",
    "    word_to_idx[word] = i\n",
    "    try: \n",
    "        weights_matrix[i] = glove[word]\n",
    "    except KeyError:\n",
    "        weights_matrix[i] = np.random.normal(scale=0.6, size=100)\n",
    "        error_count += 1\n",
    "\n",
    "# Add in \"empty\" token\n",
    "word_to_idx[\"\"] = len(target_vocab)\n",
    "        \n",
    "print(f\"We found {error_count} out of {len(target_vocab)} words not in the Glove model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "blind-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the text to have length 100\n",
    "df['words'] = df['words'].apply(lambda x: x[:100]+([\"\"]*(100-len(x[:100]))))\n",
    "\n",
    "# Encode text using the indices\n",
    "df['text_idx'] = df['words'].apply(lambda lst: np.array([word_to_idx[w] for w in lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comprehensive-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, \n",
    "                     non_trainable=False):\n",
    "    weights_matrix = torch.Tensor(weights_matrix)\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "\n",
    "    return emb_layer, num_embeddings, embedding_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-affairs",
   "metadata": {},
   "source": [
    "### FakeBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "flexible-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, targets):\n",
    "\n",
    "    predicted = [int(y_pred.detach().argmax(-1)) for y_pred in output]\n",
    "    targets = [int(y) for y in targets]\n",
    "    correct = sum(a==b for (a,b) in zip(predicted, targets))\n",
    "    accuracy = 100*correct/len(targets) \n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def train(data,\n",
    "          test,\n",
    "          weights_matrix,\n",
    "          num_epochs = 10,\n",
    "          batch_size = 100,\n",
    "          learning_rate = 0.1):\n",
    "    \n",
    "    # Instantiate model & optimization \n",
    "    model = FakeBERT(weights_matrix)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for ep in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        # Iterate over batches\n",
    "        for i in range(data.shape[0]//batch_size):\n",
    "\n",
    "            # Declare features and target labels\n",
    "            X = torch.Tensor([train_data for train_data in data['text_idx'][i*batch_size:(i+1)*batch_size]]).to(dtype=torch.long)\n",
    "            y = data['target'][i*batch_size:(i+1)*batch_size].values\n",
    "            y = torch.Tensor(y).to(dtype=torch.long)\n",
    "\n",
    "            # Get predictions from model\n",
    "            pred = model(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss_func = nn.CrossEntropyLoss()\n",
    "            loss = loss_func(pred, y)\n",
    "\n",
    "            # Backpropagate\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate model\n",
    "        model.eval()\n",
    "\n",
    "        # Prepare test data\n",
    "        test_X = torch.Tensor([test_data for test_data in test['text_idx']]).to(dtype=torch.long)\n",
    "        test_y = test['target'].values\n",
    "        test_y = torch.Tensor(test_y).to(dtype=torch.long)\n",
    "            \n",
    "        # Evaluate on test data\n",
    "        test_pred = model(test_X)\n",
    "        test_accuracy = get_accuracy(test_pred, test_y)\n",
    "        \n",
    "        # Print accuracy\n",
    "        print(f\"Test accuracy: {test_accuracy} at epoch: {ep}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "described-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeBERT(nn.Module):\n",
    "    def __init__(self, weights_matrix):\n",
    "        super(FakeBERT, self).__init__()\n",
    "        \n",
    "        # Layer 0: Embedding Layer\n",
    "        self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "\n",
    "        # Layer 1: Conv1D + Maxpool\n",
    "        self.conv_1 = nn.Conv1d(in_channels=100, out_channels=128, kernel_size=3, stride=1)\n",
    "        self.sigm_1 = nn.Sigmoid()\n",
    "        self.pool_1 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 2: Conv1D + Maxpool\n",
    "        self.conv_2 = nn.Conv1d(in_channels=100, out_channels=128, kernel_size=4, stride=1)\n",
    "        self.sigm_2 = nn.Sigmoid()\n",
    "        self.pool_2 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 3: Conv1D + Maxpool\n",
    "        self.conv_3 = nn.Conv1d(in_channels=100, out_channels=128, kernel_size=5, stride=1)\n",
    "        self.sigm_3 = nn.Sigmoid()\n",
    "        self.pool_3 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 4: Conv1D + Maxpool\n",
    "        self.conv_4 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, stride=1)\n",
    "        self.sigm_4 = nn.Sigmoid()\n",
    "        self.pool_4 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 5: Conv1D + Maxpool\n",
    "        self.conv_5 = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5, stride=1)\n",
    "        self.sigm_5 = nn.Sigmoid()\n",
    "        self.pool_5 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
    "        \n",
    "        # Layer 6: Fully Connected Layer \n",
    "        self.full_6 = nn.Linear(128,32)\n",
    "        self.sigm_6 = nn.Sigmoid()\n",
    "        \n",
    "        # Layer 7: Fully Connected Layer \n",
    "        self.full_7 = nn.Linear(32,6)\n",
    "        self.soft_7 = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate the embeddings with Glove\n",
    "        emb = self.embedding(x)\n",
    "\n",
    "        # Generate the 3 1D conv layers\n",
    "        conv_1 = self.pool_1(self.sigm_1(self.conv_1(emb)))\n",
    "        conv_2 = self.pool_2(self.sigm_2(self.conv_2(emb)))        \n",
    "        conv_3 = self.pool_3(self.sigm_3(self.conv_3(emb)))\n",
    "        \n",
    "        # Concatenate the 3 layers\n",
    "        cat = torch.cat((conv_1,conv_2,conv_3),2)\n",
    "        \n",
    "        # Pass the concatenated output through 2 1D conv layers\n",
    "        conv_4 = self.pool_4(self.sigm_4(self.conv_4(cat)))        \n",
    "        conv_5 = self.pool_5(self.sigm_5(self.conv_5(conv_4)))  \n",
    "\n",
    "        # Flatten the output\n",
    "        flat = conv_5.flatten(start_dim=1)\n",
    "\n",
    "        # Pass through 2 fully connected layers\n",
    "        full_6 = self.sigm_6(self.full_6(flat))\n",
    "        full_7 = self.soft_7(self.full_7(full_6))\n",
    "        \n",
    "        return full_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "literary-brake",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:10000].reset_index(drop=True)\n",
    "test = df.iloc[10000:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "smaller-basic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 18.5 at epoch: 0\n",
      "Test accuracy: 21.0 at epoch: 1\n",
      "Test accuracy: 21.0 at epoch: 2\n",
      "Test accuracy: 21.0 at epoch: 3\n",
      "Test accuracy: 21.0 at epoch: 4\n",
      "Test accuracy: 21.0 at epoch: 5\n",
      "Test accuracy: 21.0 at epoch: 6\n",
      "Test accuracy: 21.0 at epoch: 7\n",
      "Test accuracy: 21.0 at epoch: 8\n",
      "Test accuracy: 21.0 at epoch: 9\n"
     ]
    }
   ],
   "source": [
    "model = train(data, \n",
    "              test, \n",
    "              weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "timely-little",
   "metadata": {},
   "source": [
    "## Testing on Generated Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "practical-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df_pos = pd.read_csv(\"liar_positive.csv\")\n",
    "df_neg = pd.read_csv(\"liar_negative.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "found-casino",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings from Glove for positive statement dataset\n",
    "df_pos['target'] = df_pos['label'].apply(lambda x: label_encodings[x])\n",
    "df_pos['words'] = df_pos['statement'].apply(lambda x: x.\\\n",
    "                                            replace('?',' ?').\\\n",
    "                                            replace('.',' .').\\\n",
    "                                            lower().split())\n",
    "df_pos['words'] = df_pos['words'].apply(lambda x: x[:100]+([\"\"]*(100-len(x[:100]))))\n",
    "df_pos['text_idx'] = df_pos['words'].apply(lambda lst: np.array([word_to_idx[w] for w in lst]))\n",
    "\n",
    "# Generate embeddings from Glove for negative statement dataset\n",
    "df_neg['target'] = df_neg['label'].apply(lambda x: label_encodings[x])\n",
    "df_neg['words'] = df_neg['statement'].apply(lambda x: x.\\\n",
    "                                            replace('?',' ?').\\\n",
    "                                            replace('.',' .').\\\n",
    "                                            lower().split())\n",
    "df_neg['words'] = df_neg['words'].apply(lambda x: x[:100]+([\"\"]*(100-len(x[:100]))))\n",
    "df_neg['text_idx'] = df_neg['words'].apply(lambda lst: np.array([word_to_idx[w] for w in lst]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thousand-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix input and target for model prediction and evaluation\n",
    "input_pos = torch.Tensor([train_data for train_data in df_pos['text_idx']]).to(dtype=torch.long)\n",
    "target_pos = torch.Tensor(df_pos['target'].values).to(dtype=torch.long)\n",
    "\n",
    "input_neg = torch.Tensor([train_data for train_data in df_neg['text_idx']]).to(dtype=torch.long)\n",
    "target_neg = torch.Tensor(df_neg['target'].values).to(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eleven-validity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:62: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "pred_pos = model(input_pos)\n",
    "pred_neg = model(input_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "relative-happening",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.,   7.,  27.,  57.,  76., 134., 176., 164., 208., 218., 191.,\n",
       "        206., 233., 217., 194., 154., 175., 145., 138., 127., 123.,  97.,\n",
       "         95.,  87.,  71.,  60.,  61.,  40.,  39.,  26.,  26.,  27.,  26.,\n",
       "         15.,  24.,  17.,  12.,   9.,   9.,   7.,   5.,   3.,   5.,   4.,\n",
       "          1.,   1.,   1.,   1.,   1.,   1.,   0.,   1.,   0.,   0.,   0.,\n",
       "          1.,   1.,   1.,   0.,   1.]),\n",
       " array([0.00000000e+00, 6.91145942e-07, 1.38229188e-06, 2.07343783e-06,\n",
       "        2.76458377e-06, 3.45572971e-06, 4.14687565e-06, 4.83802160e-06,\n",
       "        5.52916754e-06, 6.22031348e-06, 6.91145942e-06, 7.60260536e-06,\n",
       "        8.29375131e-06, 8.98489725e-06, 9.67604319e-06, 1.03671891e-05,\n",
       "        1.10583351e-05, 1.17494810e-05, 1.24406270e-05, 1.31317729e-05,\n",
       "        1.38229188e-05, 1.45140648e-05, 1.52052107e-05, 1.58963567e-05,\n",
       "        1.65875026e-05, 1.72786486e-05, 1.79697945e-05, 1.86609404e-05,\n",
       "        1.93520864e-05, 2.00432323e-05, 2.07343783e-05, 2.14255242e-05,\n",
       "        2.21166702e-05, 2.28078161e-05, 2.34989620e-05, 2.41901080e-05,\n",
       "        2.48812539e-05, 2.55723999e-05, 2.62635458e-05, 2.69546917e-05,\n",
       "        2.76458377e-05, 2.83369836e-05, 2.90281296e-05, 2.97192755e-05,\n",
       "        3.04104215e-05, 3.11015674e-05, 3.17927133e-05, 3.24838593e-05,\n",
       "        3.31750052e-05, 3.38661512e-05, 3.45572971e-05, 3.52484431e-05,\n",
       "        3.59395890e-05, 3.66307349e-05, 3.73218809e-05, 3.80130268e-05,\n",
       "        3.87041728e-05, 3.93953187e-05, 4.00864647e-05, 4.07776106e-05,\n",
       "        4.14687565e-05]),\n",
       " <BarContainer object of 60 artists>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Difference between predictions\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([torch.norm(pos-neg,2).item() for (pos,neg) in zip(pred_pos, pred_neg)], bins=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aquatic-liberal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([p==n for (p,n) in zip([pred.argmax().item() for pred in pred_pos],\n",
    "[pred.argmax().item() for pred in pred_neg])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-nickel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
