{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"automodel_tutorial.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMi5oZfOflL2MkN8G31zavP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nVqe1h6QTeqU"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4-8mzRfTenc"},"source":["!pip3 install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1qS7W-lTek4","executionInfo":{"status":"ok","timestamp":1621778201419,"user_tz":-480,"elapsed":5520,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["import pandas as pd\n","import numpy as np\n","import json, re\n","from tqdm import tqdm_notebook\n","from uuid import uuid4\n","import time\n","import datetime\n","import random\n","\n","## Torch Modules\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# Transformers\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import (\n","    BertModel,\n","    BertForSequenceClassification,\n","                          BertTokenizer,\n","                          RobertaForSequenceClassification,\n","                          RobertaTokenizer,\n","                         AdamW)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmGuuteYTegb","executionInfo":{"status":"ok","timestamp":1621778211945,"user_tz":-480,"elapsed":5,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["def encode_dataframe(statement_col, target_col, unpack=False):\n","    # Tokenize statements\n","    bert_encoded_dict = statement_col.apply(lambda sent: bert_tokenizer.encode_plus(\n","                                      sent,                      # Sentence to encode.\n","                                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                                      max_length = 120,           # Pad & truncate all sentences.\n","                                      pad_to_max_length = True,\n","                                      return_attention_mask = True,   # Construct attn. masks.\n","                                      return_tensors = 'pt',     # Return pytorch tensors.\n","                                      truncation = True\n","                                ))\n","    bert_input_ids = torch.cat([item['input_ids'] for item in bert_encoded_dict], dim=0)\n","    bert_attention_masks = torch.cat([item['attention_mask'] for item in bert_encoded_dict], dim=0)\n","\n","    # Format targets\n","    labels = torch.tensor(target_col)\n","    sentence_ids = torch.tensor(range(len(target_col)))\n","\n","    # Combine the training inputs into a TensorDataset\n","    bert_dataset = TensorDataset(sentence_ids, bert_input_ids, bert_attention_masks, labels)\n","\n","    # Remove indices\n","    trial_dataset = index_remover(bert_dataset)\n","\n","    if unpack:\n","        return bert_input_ids, bert_attention_masks, labels\n","    else:\n","        return trial_dataset\n","\n","def index_remover(tensordata):\n","    input_ids = []\n","    attention_masks = []\n","    labels = []\n","   \n","    for a,b,c,d in tensordata:\n","        input_ids.append(b.tolist())\n","        attention_masks.append(c.tolist())\n","        labels.append(d.tolist())\n","        \n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","    labels = torch.tensor(labels)\n","    \n","    final_dataset = TensorDataset(input_ids, attention_masks, labels)\n","    return final_dataset\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"-erJMaZDTeXr","executionInfo":{"status":"ok","timestamp":1621778215726,"user_tz":-480,"elapsed":1075,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["import torch.nn as nn\n","from transformers import AutoModel\n","class FakeBERT(nn.Module):\n","    def __init__(self):\n","        super(FakeBERT, self).__init__()\n","        \n","        self.base_model = AutoModel.from_pretrained('bert-base-uncased')\n","\n","        # Layer 1: Conv1D + Maxpool\n","        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n","        self.sigm_1 = nn.ReLU()\n","        self.pool_1 = nn.MaxPool1d(kernel_size=5, stride=5)\n","        \n","        # Layer 6: Fully Connected Layer \n","        self.full_6 = nn.Linear(153,32)\n","        self.sigm_6 = nn.Sigmoid()\n","        \n","        # Layer 7: Fully Connected Layer \n","        self.full_7 = nn.Linear(32,2)\n","        self.soft_7 = nn.Softmax()\n","\n","    def forward(self, input_ids, attn_mask):\n","        bert_output = self.base_model(input_ids, attention_mask=attn_mask)\n","        outputs = self.pool_1(self.sigm_1(self.conv_1(bert_output['pooler_output'].unsqueeze(1))))\n","        outputs = self.sigm_6(self.full_6(outputs))\n","        outputs = self.soft_7(self.full_7(outputs))\n","        return outputs, bert_output\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"OiYXWrijGxfu"},"source":["# Device\n","device = torch.device(\"cuda:0\")\n","\n","# BERT\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbg-euxIESKb","executionInfo":{"status":"ok","timestamp":1621779698692,"user_tz":-480,"elapsed":1029,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["# Load in dataset\n","\n","# df_train = pd.read_csv(\"/content/drive/MyDrive/fake-news-explainability/fake_news_train.csv\")\n","# df_train = df_train.dropna(subset=['text']).reset_index(drop=True)\n","# df_train['target'] = df_train['label']\n","# df_train_encode = encode_dataframe(df_train['text'], df_train['target'])\n","# torch.save(df_train_encode,\n","#            \"/content/drive/MyDrive/fake-news-explainability/fake_news_encoded.pt\")\n","\n","df_train_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/fake_news_encoded.pt\")"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"iK2qD5UTEDJx","executionInfo":{"status":"ok","timestamp":1621779699214,"user_tz":-480,"elapsed":3,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["# Load data into dataloader\n","batch_size = 32\n","bert_train_dataloader = DataLoader(\n","    df_train_encode,  # The training samples.\n","    batch_size = batch_size # Trains with this batch size.\n","    )"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5Zo7VzbLDA2","executionInfo":{"status":"ok","timestamp":1621779703738,"user_tz":-480,"elapsed":2588,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"1e5307b7-5246-4d8f-ba89-7a4ee0588a5f"},"source":["# Load model\n","bert_model = FakeBERT().to(device)\n","bert_training_stats = []\n","epochs = 3\n","total_steps = len(bert_train_dataloader) * epochs\n","loss_func = nn.CrossEntropyLoss()\n","\n","# Optimizer\n","bert_optimizer = AdamW(bert_model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","# Create the learning rate scheduler.\n","bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n","                                                 num_warmup_steps = 0, # Default value in run_glue.py\n","                                                 num_training_steps = total_steps)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ICr28_jdTlHC"},"source":["# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    t0 = time.time()\n","    total_train_loss = 0\n","    bert_model.train()\n","\n","    for step, batch in enumerate(bert_train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n","\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Zero grads\n","        bert_model.zero_grad()        \n","\n","        # Forward pass\n","        output, bert_output = bert_model(b_input_ids, b_input_mask)\n","        \n","        # Accumulate loss\n","        loss = loss_func(output.squeeze(1), \n","                         b_labels)\n","        \n","        # Backpropagate\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The bert_optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        bert_optimizer.step()\n","\n","        # Update the learning rate.\n","        # bert_scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(bert_train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","\n","    # Record all statistics from this epoch.\n","    bert_training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            # 'Valid. Loss': avg_val_loss,\n","            # 'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            # 'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0rAmeKL16WN","executionInfo":{"status":"ok","timestamp":1621784130429,"user_tz":-480,"elapsed":307078,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"a2b6c826-e787-4585-e488-1421ac577a71"},"source":["# Generate predictions\n","results = []\n","with torch.no_grad():\n","    for step, batch in enumerate(bert_train_dataloader):\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)       \n","\n","        # Forward pass\n","        output, bert_output = bert_model(b_input_ids, b_input_mask)\n","        results.append(output)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJoDmN0N5VYJ","executionInfo":{"status":"ok","timestamp":1621784960634,"user_tz":-480,"elapsed":512,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"80a44e71-a01d-4027-ea47-2e27cbcda67b"},"source":["# Calculate accuracy\n","results_ = torch.vstack(results).squeeze(1)\n","sum(torch.argmax(results_.cpu(), axis=1) == df_train_encode.tensors[2])/len(df_train_encode.tensors[2])"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.9913)"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"1ibbLn-37ZAK","executionInfo":{"status":"ok","timestamp":1621785282536,"user_tz":-480,"elapsed":2469,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["# Save model\n","torch.save(bert_model.state_dict(),\n","           \"/content/drive/MyDrive/fake-news-explainability/bert_model_fake_news_kaggle\")\n","torch.save(bert_model, \n","           \"/content/drive/MyDrive/fake-news-explainability/bert_model_fake_news_kaggle_full\")\n"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"2S3uIV7F8Jwa"},"source":["# Load model\n","model = FakeBERT()\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/fake-news-explainability/bert_model_fake_news_kaggle\"))\n","model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yzZz0Jfd-ION"},"source":[""],"execution_count":null,"outputs":[]}]}