{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"modified_fakebert.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"nVqe1h6QTeqU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622525699600,"user_tz":-480,"elapsed":17944,"user":{"displayName":"Lorenzo Flores","photoUrl":"","userId":"01486192851255545065"}},"outputId":"8e2c8174-bd60-4715-cf9d-a1d6e59abbd8"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"a4-8mzRfTenc"},"source":["!pip3 install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1qS7W-lTek4"},"source":["import pandas as pd\n","import numpy as np\n","import json, re\n","from tqdm import tqdm_notebook\n","from uuid import uuid4\n","import time\n","import datetime\n","import random\n","\n","## Torch Modules\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# Transformers\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import (\n","    BertModel,\n","    BertForSequenceClassification,\n","    BertTokenizer,\n","    RobertaForSequenceClassification,\n","    RobertaTokenizer,\n","    AdamW\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmGuuteYTegb"},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''Takes a time in seconds and returns a string hh:mm:ss'''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-erJMaZDTeXr"},"source":["import torch.nn as nn\n","from transformers import AutoModel\n","class FakeBERT(nn.Module):\n","    def __init__(self):\n","        super(FakeBERT, self).__init__()\n","        \n","        self.base_model = AutoModel.from_pretrained('bert-base-uncased')\n","\n","        # Layer 1: Conv1D + Maxpool\n","        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n","        self.sigm_1 = nn.ReLU()\n","        self.pool_1 = nn.MaxPool1d(kernel_size=5, stride=5)\n","        \n","        # Layer 6: Fully Connected Layer \n","        self.full_6 = nn.Linear(153,32)\n","        self.sigm_6 = nn.Sigmoid()\n","        \n","        # Layer 7: Fully Connected Layer \n","        self.full_7 = nn.Linear(32,2)\n","        self.soft_7 = nn.Softmax()\n","\n","    def forward(self, input_ids, attn_mask):\n","        bert_output = self.base_model(input_ids, attention_mask=attn_mask)\n","        bert_output = bert_output['pooler_output'].unsqueeze(1)\n","        # bert_perturb = 0.1*torch.rand(768).to(device)\n","        # bert_output = bert_output + bert_perturb\n","        outputs = self.pool_1(self.sigm_1(self.conv_1(bert_output)))\n","        outputs = self.sigm_6(self.full_6(outputs))\n","        outputs = self.soft_7(self.full_7(outputs))\n","        return outputs, bert_output\n","\n","def train():\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        t0 = time.time()\n","        total_train_loss = 0\n","        bert_model.train()\n","        step = 0\n","\n","        for (batch_pos,batch_neg) in zip(bert_train_pos_dataloader,\n","                                         bert_train_neg_dataloader):\n","\n","            # if step%40==0:\n","            #     elapsed = format_time(time.time() - t0)\n","            #     print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_pos_dataloader), elapsed))\n","\n","            # Unpack batches\n","            b_input_ids_pos  = batch_pos[0].to(device)\n","            b_input_mask_pos = batch_pos[1].to(device)\n","            b_labels_pos     = batch_pos[2].to(device)\n","            b_input_ids_neg  = batch_neg[0].to(device)\n","            b_input_mask_neg = batch_neg[1].to(device)\n","            b_labels_neg     = batch_neg[2].to(device)\n","\n","            # Zero grads\n","            bert_model.zero_grad()        \n","\n","            # Forward pass\n","            output_pos, _ = bert_model(b_input_ids_pos, b_input_mask_pos)\n","            output_neg, _ = bert_model(b_input_ids_neg, b_input_mask_neg)\n","\n","            # Accumulate loss\n","            output_pos, output_neg = output_pos.squeeze(1), output_neg.squeeze(1)\n","            loss = loss_func(output_pos, b_labels_pos)\n","            loss += loss_func(output_neg, b_labels_neg)\n","            loss += (2-torch.mean(torch.norm(output_pos-output_neg,p=2,dim=1)).detach())\n","            \n","            if step%40==0:\n","                elapsed = format_time(time.time() - t0)\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_pos_dataloader), elapsed))\n","                print(loss)\n","\n","            # Backpropagate\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The bert_optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            bert_optimizer.step()\n","\n","            # Update the learning rate.\n","            bert_scheduler.step()\n","\n","            # Increment step\n","            step += 1\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(bert_train_pos_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epoch took: {:}\".format(training_time))\n","\n","        # Record all statistics from this epoch.\n","        bert_training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                # 'Valid. Loss': avg_val_loss,\n","                # 'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                # 'Validation Time': validation_time\n","            }\n","        )\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OiYXWrijGxfu"},"source":["# Device\n","device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","# BERT\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9a-6wuweTwDo"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"iK2qD5UTEDJx"},"source":["# Load encoded Fake-News Kaggle dataset\n","df_pos_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/fake_news_pos_encoded.pt\")\n","df_neg_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/fake_news_neg_encoded.pt\")\n","\n","# Load data into dataloader\n","batch_size = 32\n","bert_train_pos_dataloader = DataLoader(\n","    df_pos_encode,  # The training samples.\n","    batch_size = batch_size # Trains with this batch size.\n","    )\n","bert_train_neg_dataloader = DataLoader(\n","    df_neg_encode,  # The training samples.\n","    batch_size = batch_size # Trains with this batch size.\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rA8qI6hZCbzL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622526028061,"user_tz":-480,"elapsed":1889,"user":{"displayName":"Lorenzo Flores","photoUrl":"","userId":"01486192851255545065"}},"outputId":"785b7ea5-d48e-488d-ca06-e9ffa61a2fbc"},"source":["# Load model\n","bert_model = FakeBERT().to(device)\n","bert_training_stats = []\n","epochs = 2\n","total_steps = len(bert_train_pos_dataloader) * epochs\n","loss_func = nn.CrossEntropyLoss()\n","\n","# Optimizer\n","bert_optimizer = AdamW(bert_model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","# Create the learning rate scheduler.\n","bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n","                                                 num_warmup_steps = 0, # Default value in run_glue.py\n","                                                 num_training_steps = total_steps)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"FrEft0thUO03"},"source":["# Train\n","train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o04bpssnUkXG"},"source":["# Save model\n","torch.save(bert_model.state_dict(),\n","           \"/content/drive/MyDrive/fake-news-explainability/Models/bert_model_fake_news_test_kaggle\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nqa-BzdNUDrY"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"id":"e9gXekUDln2Q"},"source":["# Load model\n","bert_model = FakeBERT().to(device)\n","bert_model.load_state_dict(torch.load(\"/content/drive/MyDrive/fake-news-explainability/Models/bert_model_fake_news_test_kaggle\"))\n","\n","# Load data\n","df_train_pos_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/fake_news_pos_encoded.pt\")\n","df_train_neg_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/fake_news_neg_encoded.pt\")\n","\n","# Load data into dataloader\n","batch_size = 32\n","bert_train_pos_dataloader = DataLoader(\n","    df_train_pos_encode,  # The training samples.\n","    batch_size = batch_size # Trains with this batch size.\n","    )\n","bert_train_neg_dataloader = DataLoader(\n","    df_train_neg_encode,  # The training samples.\n","    batch_size = batch_size # Trains with this batch size.\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qkXSVx_o0WpZ","executionInfo":{"status":"ok","timestamp":1622523937905,"user_tz":-480,"elapsed":135640,"user":{"displayName":"Lorenzo Flores","photoUrl":"","userId":"01486192851255545065"}},"outputId":"a9403c43-58cd-4793-fb2f-e97bca8c801e"},"source":["# Generate predictions\n","results = []\n","with torch.no_grad():\n","    for step, batch in enumerate(bert_train_pos_dataloader):\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)       \n","\n","        # Forward pass\n","        output, bert_output = bert_model(b_input_ids, b_input_mask)\n","        results.append(output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NJoDmN0N5VYJ","executionInfo":{"status":"ok","timestamp":1622515174877,"user_tz":-480,"elapsed":316,"user":{"displayName":"Lorenzo Flores","photoUrl":"","userId":"01486192851255545065"}},"outputId":"c3bb5c1d-aef9-4867-d807-dd7af0a748f8"},"source":["# Calculate accuracy\n","labels = df_pos_encode.tensors[2]\n","results_ = torch.vstack(results).squeeze(1)\n","sum(torch.argmax(results_.cpu(), axis=1) == labels)/len(labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5020)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d6twaJN-nj5H","executionInfo":{"status":"ok","timestamp":1622524097500,"user_tz":-480,"elapsed":146852,"user":{"displayName":"Lorenzo Flores","photoUrl":"","userId":"01486192851255545065"}},"outputId":"41347c9a-7c6c-44b0-ff27-b7e1e3ecdf56"},"source":["# Generate predictions\n","results2 = []\n","with torch.no_grad():\n","    for step, batch in enumerate(bert_train_neg_dataloader):\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)       \n","\n","        # Forward pass\n","        output, bert_output = bert_model(b_input_ids, b_input_mask)\n","        results2.append(output)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MIKXckQ7nkIG","executionInfo":{"status":"ok","timestamp":1622515332542,"user_tz":-480,"elapsed":7,"user":{"displayName":"Lorenzo Flores","photoUrl":"","userId":"01486192851255545065"}},"outputId":"2b627e19-fe91-4ee3-bcab-1e1ca9619b57"},"source":["# Calculate accuracy\n","labels = df_neg_encode.tensors[2]\n","results_ = torch.vstack(results).squeeze(1)\n","sum(torch.argmax(results_.cpu(), axis=1) == labels)/len(labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.5010)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"DNOPsKJ1AJCe"},"source":["results = torch.vstack(results).squeeze(1)\n","results2 = torch.vstack(results2).squeeze(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXoy6tHa92Fp"},"source":["## Results Log\n","* FakeBERT, Trained on positive and negative classes\n","  * Result on positive/negative: tensor(0.9990)\n","  * Result on fake-news dataset: ???\n","\n"]},{"cell_type":"code","metadata":{"id":"yzZz0Jfd-ION"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_pZZxPcNsr8f"},"source":[""],"execution_count":null,"outputs":[]}]}