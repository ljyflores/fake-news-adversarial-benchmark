{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fakebert.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nVqe1h6QTeqU"},"source":["from google.colab import drive, files\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4-8mzRfTenc"},"source":["!pip3 install transformers\n","!cp /content/drive/MyDrive/fake-news-explainability/utils_fake_news.py ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p1qS7W-lTek4"},"source":["import pandas as pd\n","import numpy as np\n","import json, re\n","from tqdm import tqdm_notebook\n","from uuid import uuid4\n","import time\n","import datetime\n","import random\n","import itertools\n","import os\n","\n","## Torch Modules\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import (\n","    Dataset, \n","    DataLoader,\n","    TensorDataset, \n","    random_split, \n","    RandomSampler, \n","    SequentialSampler)\n","\n","# Transformers\n","from transformers import (\n","    BertForSequenceClassification,\n","    BertTokenizer,\n","    RobertaForSequenceClassification,\n","    RobertaTokenizer,\n","    AdamW,\n","    get_linear_schedule_with_warmup)\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","\n","%run utils_fake_news.py\n","\n","# Device\n","device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aWKDP6yz2DI9"},"source":["## Model & Training Function"]},{"cell_type":"code","metadata":{"id":"4bBYr4RA2CrJ"},"source":["import torch.nn as nn\n","from transformers import AutoModel\n","class FakeBERT(nn.Module):\n","    def __init__(self):\n","        super(FakeBERT, self).__init__()\n","        \n","        self.base_model = AutoModel.from_pretrained('bert-base-uncased')\n","\n","        # Layer 1: Conv1D + Maxpool\n","        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n","        self.sigm_1 = nn.ReLU()\n","        self.pool_1 = nn.MaxPool1d(kernel_size=5, stride=5)\n","\n","        # Layer 2: Conv1D + Maxpool\n","        self.conv_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=4, stride=1)\n","        self.sigm_2 = nn.ReLU()\n","        self.pool_2 = nn.MaxPool1d(kernel_size=5, stride=5)\n","\n","        # Layer 5: Fully Connected Layer \n","        self.full_5 = nn.Linear(306,128)\n","        self.sigm_5 = nn.Sigmoid()\n","\n","        # Layer 5: Fully Connected Layer \n","        self.full_6 = nn.Linear(128,32)\n","        self.sigm_6 = nn.Sigmoid()\n","        \n","        # Layer 7: Fully Connected Layer \n","        self.full_7 = nn.Linear(32,2)\n","        self.soft_7 = nn.Softmax()\n","\n","    def forward(self, input_ids, attn_mask):\n","        bert_output = self.base_model(input_ids, attention_mask=attn_mask)\n","        bert_output = bert_output['pooler_output'].unsqueeze(1)\n","\n","        output1 = self.pool_1(self.sigm_1(self.conv_1(bert_output))).squeeze(1)\n","        output2 = self.pool_2(self.sigm_2(self.conv_2(bert_output))).squeeze(1)\n","        outputs = torch.cat((output1,output2),dim=1)\n","        outputs = self.sigm_5(self.full_5(outputs))\n","        outputs = self.sigm_6(self.full_6(outputs))\n","        outputs = self.soft_7(self.full_7(outputs))\n","        return outputs\n","\n","def train():\n","    total_t0 = time.time()\n","    for epoch_i in range(0, epochs):\n","        \n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        t0 = time.time()\n","        total_train_loss = 0\n","        bert_model.train()\n","\n","        for step, batch in enumerate(bert_train_dataloader):\n","\n","            # Unpack batch\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Zero grads\n","            bert_model.zero_grad()        \n","\n","            # Forward pass\n","            output = bert_model(b_input_ids, b_input_mask)\n","            \n","            # Accumulate loss\n","            loss = loss_func(output.squeeze(1), b_labels)\n","\n","            # Backpropagate\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The bert_optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            bert_optimizer.step()\n","\n","            # Update the learning rate.\n","            bert_scheduler.step()\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","                elapsed = format_time(time.time() - t0)\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n","                print(loss.detach())\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(bert_train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epoch took: {:}\".format(training_time))\n","\n","        # Record all statistics from this epoch.\n","        bert_training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                # 'Valid. Loss': avg_val_loss,\n","                # 'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                # 'Validation Time': validation_time\n","            }\n","        )\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eFptuf6I7oFC"},"source":["## Load Data"]},{"cell_type":"code","metadata":{"id":"BYWfkdPw7n52"},"source":["# Load encoded Fake-News Kaggle dataset\n","df_train_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/training/fake_news.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rn4E6EnF7nuY"},"source":["# Load data into dataloader\n","batch_size = 32\n","bert_train_dataloader = DataLoader(\n","    df_train_encode,  # The training samples.\n","    batch_size = batch_size # Trains with this batch size.\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9a-6wuweTwDo"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"qTA3a85IDGYM"},"source":["# BERT\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# Model\n","bert_model = FakeBERT().to(device)\n","\n","# Optimizer\n","bert_optimizer = AdamW(bert_model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","# Training Params\n","bert_training_stats = []\n","epochs = 2\n","total_steps = len(bert_train_dataloader) * epochs\n","loss_func = nn.CrossEntropyLoss()\n","\n","# Learning rate scheduler.\n","bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n","                                                 num_warmup_steps = 0, # Default value in run_glue.py\n","                                                 num_training_steps = total_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FrEft0thUO03","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626139493828,"user_tz":-480,"elapsed":866801,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"56fa54cf-2ac8-4bfa-8031-89ec554dbc7e"},"source":["# Train or load pre-trained\n","bert_model_path = \"/content/drive/MyDrive/fake-news-explainability/Models/fake_news_model\"\n","\n","if os.path.exists('none'):\n","    bert_model = FakeBERT().to(device)\n","    bert_model.load_state_dict(torch.load(bert_model_path))\n","else:\n","    train()\n","    torch.save(bert_model.state_dict(), bert_model_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["======== Epoch 1 / 2 ========\n","Training...\n","  Batch    40  of    633.    Elapsed: 0:00:25.\n","tensor(0.6948, device='cuda:0')\n","  Batch    80  of    633.    Elapsed: 0:00:51.\n","tensor(0.6505, device='cuda:0')\n","  Batch   120  of    633.    Elapsed: 0:01:17.\n","tensor(0.6313, device='cuda:0')\n","  Batch   160  of    633.    Elapsed: 0:01:44.\n","tensor(0.5944, device='cuda:0')\n","  Batch   200  of    633.    Elapsed: 0:02:11.\n","tensor(0.5767, device='cuda:0')\n","  Batch   240  of    633.    Elapsed: 0:02:37.\n","tensor(0.5653, device='cuda:0')\n","  Batch   280  of    633.    Elapsed: 0:03:04.\n","tensor(0.5254, device='cuda:0')\n","  Batch   320  of    633.    Elapsed: 0:03:31.\n","tensor(0.5387, device='cuda:0')\n","  Batch   360  of    633.    Elapsed: 0:03:58.\n","tensor(0.4741, device='cuda:0')\n","  Batch   400  of    633.    Elapsed: 0:04:25.\n","tensor(0.5171, device='cuda:0')\n","  Batch   440  of    633.    Elapsed: 0:04:52.\n","tensor(0.5391, device='cuda:0')\n","  Batch   480  of    633.    Elapsed: 0:05:18.\n","tensor(0.5172, device='cuda:0')\n","  Batch   520  of    633.    Elapsed: 0:05:45.\n","tensor(0.4599, device='cuda:0')\n","  Batch   560  of    633.    Elapsed: 0:06:12.\n","tensor(0.4735, device='cuda:0')\n","  Batch   600  of    633.    Elapsed: 0:06:39.\n","tensor(0.4772, device='cuda:0')\n","\n","  Average training loss: 0.00\n","  Training epoch took: 0:07:00\n","======== Epoch 2 / 2 ========\n","Training...\n","  Batch    40  of    633.    Elapsed: 0:00:27.\n","tensor(0.4291, device='cuda:0')\n","  Batch    80  of    633.    Elapsed: 0:00:54.\n","tensor(0.4199, device='cuda:0')\n","  Batch   120  of    633.    Elapsed: 0:01:21.\n","tensor(0.4614, device='cuda:0')\n","  Batch   160  of    633.    Elapsed: 0:01:48.\n","tensor(0.4171, device='cuda:0')\n","  Batch   200  of    633.    Elapsed: 0:02:15.\n","tensor(0.4566, device='cuda:0')\n","  Batch   240  of    633.    Elapsed: 0:02:42.\n","tensor(0.4785, device='cuda:0')\n","  Batch   280  of    633.    Elapsed: 0:03:08.\n","tensor(0.4332, device='cuda:0')\n","  Batch   320  of    633.    Elapsed: 0:03:35.\n","tensor(0.4550, device='cuda:0')\n","  Batch   360  of    633.    Elapsed: 0:04:02.\n","tensor(0.4044, device='cuda:0')\n","  Batch   400  of    633.    Elapsed: 0:04:29.\n","tensor(0.4754, device='cuda:0')\n","  Batch   440  of    633.    Elapsed: 0:04:56.\n","tensor(0.5154, device='cuda:0')\n","  Batch   480  of    633.    Elapsed: 0:05:22.\n","tensor(0.4927, device='cuda:0')\n","  Batch   520  of    633.    Elapsed: 0:05:49.\n","tensor(0.4261, device='cuda:0')\n","  Batch   560  of    633.    Elapsed: 0:06:16.\n","tensor(0.4483, device='cuda:0')\n","  Batch   600  of    633.    Elapsed: 0:06:43.\n","tensor(0.4544, device='cuda:0')\n","\n","  Average training loss: 0.00\n","  Training epoch took: 0:07:04\n","\n","Training complete!\n","Total training took 0:14:04 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Nqa-BzdNUDrY"},"source":["## Evaluate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FANxOr7uSapT","executionInfo":{"status":"ok","timestamp":1626139886182,"user_tz":-480,"elapsed":159294,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"8cceb6bf-1b4d-48d8-ce86-b4057a974761"},"source":["# Load encoded tensors\n","df_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/training/fake_news.pt\")\n","\n","# Load data into dataloader\n","batch_size = 32\n","bert_dataloader = DataLoader(df_encode, batch_size = batch_size)\n","\n","# Generate predictions\n","outputs = []\n","with torch.no_grad():\n","    for step, batch in enumerate(bert_dataloader):\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Forward pass\n","        output = bert_model(b_input_ids, b_input_mask)\n","        outputs.append(output)        \n","        \n","# Stack outputs\n","outputs = torch.vstack(outputs)\n","\n","# Accuracy\n","print(f\"Acc: {flat_accuracy(outputs.detach().cpu().numpy(), df_encode.tensors[2].to('cpu').numpy())}\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["Acc: 0.9101373382076869\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qbnZFGfQ80Au"},"source":["# Load encoded tensors\n","df_pos_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_pos.pt\")\n","df_neg_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_neg.pt\")\n","\n","# Load data into dataloader\n","batch_size = 32\n","bert_pos_dataloader = DataLoader(df_pos_encode, batch_size = batch_size)\n","bert_neg_dataloader = DataLoader(df_neg_encode, batch_size = batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NJoDmN0N5VYJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626140040481,"user_tz":-480,"elapsed":62801,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"a9840fa8-5cbe-4798-919d-f55edf79a218"},"source":["# Generate predictions\n","# Can't input everything directly due to RAM issues\n","outputs_pos = []\n","outputs_neg = []\n","with torch.no_grad():\n","    for step, batch in enumerate(bert_pos_dataloader):\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Forward pass\n","        output = bert_model(b_input_ids, b_input_mask)\n","        outputs_pos.append(output)\n","        \n","    for step, batch in enumerate(bert_neg_dataloader):\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Forward pass\n","        output = bert_model(b_input_ids, b_input_mask)\n","        outputs_neg.append(output)\n","        \n","# Stack outputs\n","outputs_pos = torch.vstack(outputs_pos)\n","outputs_neg = torch.vstack(outputs_neg)\n","\n","# Positive statement accuracy\n","print(f\"Positive Acc: {flat_accuracy(outputs_pos.detach().cpu().numpy(), df_pos_encode.tensors[2].to('cpu').numpy())}\")\n","\n","# Negative statement accuracy\n","print(f\"Negative Acc: {flat_accuracy(outputs_neg.detach().cpu().numpy(), df_neg_encode.tensors[2].to('cpu').numpy())}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["Positive Acc: 0.9337474120082816\n","Negative Acc: 0.06599378881987578\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AneCIlz1AWOt"},"source":["## Negation"]},{"cell_type":"code","metadata":{"id":"4Ib6MvgQAcs-","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1626140045044,"user_tz":-480,"elapsed":333,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"8dc4ce75-73c7-4d4f-9934-70a1818e015a"},"source":["# Confusion matrix\n","cf_matrix = confusion_matrix(torch.argmax(outputs_pos.cpu(), axis=1), \n","                             torch.argmax(outputs_neg.cpu(), axis=1))\n","sns.heatmap(cf_matrix, annot=True)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fc0a3034910>"]},"metadata":{"tags":[]},"execution_count":33},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAWcAAAD4CAYAAAAw/yevAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUlUlEQVR4nO3de5RV1Z3g8e+vqpCHSQQCEgVafNCZ1nGixkbT9iS6YnzODJrOoKZH6TTpchJdYpvO0jyME+3EdLfYIRPbNBp8JFHCmolLYmsMkmTUjgpou3xgG4mKFCAgIEGrEKrunj/qQK5QT6iquzn1/bDOqnv32ffufaniVz9+Z59zIqWEJCkvdbWegCRpdwZnScqQwVmSMmRwlqQMGZwlKUMN/T3AW1881+Ug2s3I2YtrPQVlqHXbqtjb99j+xss9jjlDxhy21+P1FzNnScpQv2fOkjSgKm21nkGfMDhLKpe21lrPoE8YnCWVSkqVWk+hTxicJZVLxeAsSfkxc5akDHlAUJIyZOYsSflJrtaQpAx5QFCSMmRZQ5Iy5AFBScqQmbMkZcgDgpKUIQ8ISlJ+UrLmLEn5seYsSRmyrCFJGTJzlqQMtW2v9Qz6hMFZUrmUpKzhDV4llUuq9HzrQkRMjIhfRsSyiHg+ImYW7aMjYmFEvFR8HVW0R0R8JyKWR8QzEXFc1XtNL/q/FBHTe/IxDM6SyqVS6fnWtVbgCymlI4ETgUsi4kjgKmBRSmkysKh4DnAmMLnYGoGboT2YA9cAJwBTgGt2BPSuGJwllUsfBeeU0pqU0lPF4y3AC8B4YCpwR9HtDuCc4vFU4M7U7nFgZEQcBJwOLEwpbUwpbQIWAmd09zGsOUsqldSLA4IR0Uh7lrvDnJTSnA76TQKOBZ4AxqWU1hS7XgfGFY/HAyurXtZUtHXW3iWDs6Ry6cVSuiIQ7xaMq0XEe4D/C1yeUvpdRFS/PkVE2sOZdsmyhqRy6buaMxExhPbA/KOU0k+K5rVFuYLi67qifRUwserlE4q2ztq7ZHCWVC59t1ojgO8DL6SUbqzatQDYseJiOnBvVftFxaqNE4HNRfnjQeC0iBhVHAg8rWjrkmUNSeXSd+ucTwIuBJ6NiKeLti8D3wLmR8QMYAUwrdh3P3AWsBxoBj4DkFLaGBHXAUuKftemlDZ2N7jBWVK59NHp2ymlR4HoZPfHO+ifgEs6ea+5wNzejG9wllQurV5sX5Ly44WPJClDJbm2hsFZUrmYOUtShsycJSlDZs6SlCFXa0hShlK/XOpiwBmcJZWLNWdJypDBWZIy5AFBScpQW1utZ9AnDM6SysWyhiRlyOAsSRmy5ixJ+UkV1zlLUn4sa0hShlytIUkZMnOWpAwZnMtv6H+/lPojjye9tZmWWTM77FN/2FHsN3UG1NXD21to+d5X927Q+gaGnj+T+gmHk5q3sPWHN5A2radu4mSGfupzO7ttW/hj2p57Yu/GUk3dMmcWZ591KuvWv8Exx+52v1DtqZJc+Kiu1hPI2falv2Drrdd23mHYCIZ+8mK23vZNWmbNZOsP/qHH7x2jxjL8f163W3vDlFOh5W2a/+7zbH/4p+x31kUAVF5fQcvsv6HlH69g663XMfTPPgd1fvv2ZXfeOZ+z/8uf13oa5VOp9HzLWLeZc0T8B2AqML5oWgUsSCm90J8Ty0HllWXEqLGd7m849qO0Pvs46c03AEhvb/79vuM+xpCTzoaGBiqv/YZ3fjKnR+svG46awraFPwag9dlfM/Tcv+IdgO3bqjoNKU12MJg98ugTHHLIhFpPo3wGw1K6iLgSuACYBywumicAd0fEvJTSt/p5flmrG3sw1De0Z8BDh7P90ftoffJXxIETaPjQSbTc9CWotDH03EYajvsorU/+qtv3jAPevzPYU6mQtjbDiPdC85b20sa0S6kbNZat82Zn/5tfqolBslpjBnBUSml7dWNE3Ag8D3QYnCOiEWgEmP2JY/jLD03a+5nmqK6e+vGH0fLP18CQ/Rhx6bdoW/EbGo44mrrxhzN8ZnuZIxr2I73VnlUPm34lMXocUd9AjBzD8L++EYDtj9xH69JfdDlcZeVLtMyaSRw4gWHnXUbLvz8Frdu7fI002KSSJC3dBecKcDCwYpf2g4p9HUopzQHmALz1xXPL8X+MDqTNG2hr3gLb34Ht79D2yjLqDp4EEbQ++Uu2PfDD3V6z9Y6/A9przsPOu4yW712923vGyDGkzRugro4YNgKat7y7z7om0rat1H3gD6g0/bbfPp+0TypJWaO7I0qXA4si4oGImFNsPwMWAR0vXxhEWp9fTN2kP2o/MDdkP+r+4A9Ja5tofekZGo7+CLH/Ae0dh7+HGNl57bpa27IlDPnwKQA0HP0ntC5/FoAYdeDOA4Axcix1Y8dT2biu7z+UtK9LlZ5vGesyc04p/Swi/hCYwrsPCC5JKZWjsNOFoZ++gvrDjyL2fx8jvnIL234+D+rb/8paH3+QtK6Jthf/jRFXfJuUEq1PLKSy9jUAtj14F8Mar4EIaGvjnXvmkN5c3+2Y2xc/xLDzL2fElf9Ean6LrT+aBUD9oX/EkFM+CZU2qFR4555/3i2j1r7lhz+4iY999COMGTOaV19eytevvYHbbp9X62nt+0qSOUfq56P+ZS5raM+NnL24+04adFq3rYq9fY+3v3Z+j2PO/tfO2+vx+osnoUgql8zLFT1lcJZULiUpaxicJZXKYFlKJ0n7FjNnScqQwVmSMjRITt+WpH2K9xCUpBwZnCUpQ67WkKQMlSRz9lYaksqlknq+dSMi5kbEuoh4rqrtf0XEqoh4utjOqtr3pYhYHhEvRsTpVe1nFG3LI+KqnnwMM2dJpZLa+rSscTvwXeDOXdr/MaV0Q3VDRBwJnA8cRfullh8qLhwHcBPwCaAJWBIRC1JKy7oa2OAsqVz6sKyRUno4Iib1sPtUYF5K6R3glYhYTvsVPQGWp5ReBoiIeUXfLoOzZQ1JpZIqqcfbXrg0Ip4pyh6jirbxwMqqPk1FW2ftXTI4SyqXXtScI6IxIpZWbY09GOFm4HDgGGANMKs/PoZlDUnl0ouSc/Ut9XrxmrU7HkfELcB9xdNVwMSqrhOKNrpo75SZs6RSSa2VHm97IiIOqnp6LrBjJccC4PyIGBoRhwKTgcXAEmByRBwaEfvRftBwQXfjmDlLKpc+XKwREXcDJwNjIqIJuAY4OSKOARLwKnAxQErp+YiYT/uBvlbgkh2384uIS4EHgXpgbkrp+e7GNjhLKpW+vLZGSumCDpq/30X/bwDf6KD9fuD+3oxtcJZULuU4e9vgLKlcvCqdJOXIzFmS8pNaaz2DvmFwllQqycxZkjJkcJak/Jg5S1KGDM6SlKHUFrWeQp8wOEsqFTNnScpQqpg5S1J2zJwlKUMpmTlLUnbMnCUpQxVXa0hSfjwgKEkZMjhLUoZSOS7nbHCWVC5mzpKUIZfSSVKG2lytIUn5MXOWpAxZc5akDLlaQ5IyZOYsSRlqq9TVegp9wuAsqVQsa0hShiqu1pCk/LiUTpIyZFmjh0bOXtzfQ2gf1LL6kVpPQSVlWUOSMuRqDUnKUEmqGgZnSeViWUOSMuRqDUnKUEluvm1wllQuCTNnScpOq2UNScpPWTLnciwIlKRCpRdbdyJibkSsi4jnqtpGR8TCiHip+DqqaI+I+E5ELI+IZyLiuKrXTC/6vxQR03vyOQzOkkolET3eeuB24Ixd2q4CFqWUJgOLiucAZwKTi60RuBnagzlwDXACMAW4ZkdA74rBWVKp9GXmnFJ6GNi4S/NU4I7i8R3AOVXtd6Z2jwMjI+Ig4HRgYUppY0ppE7CQ3QP+bqw5SyqVtv6vOY9LKa0pHr8OjCsejwdWVvVrKto6a++SmbOkUqlEz7eIaIyIpVVbY2/GSikl+umMcTNnSaVS6UXmnFKaA8zp5RBrI+KglNKaomyxrmhfBUys6jehaFsFnLxL+6+6G8TMWVKppF5se2gBsGPFxXTg3qr2i4pVGycCm4vyx4PAaRExqjgQeFrR1iUzZ0ml0penb0fE3bRnvWMioon2VRffAuZHxAxgBTCt6H4/cBawHGgGPgOQUtoYEdcBS4p+16aUdj3IuBuDs6RSqUTfHRBMKV3Qya6Pd9A3AZd08j5zgbm9GdvgLKlU2mo9gT5icJZUKpVynL1tcJZULr1ZrZEzg7OkUvE2VZKUIcsakpQh74QiSRlqM3OWpPyYOUtShgzOkpShktxC0OAsqVzMnCUpQ56+LUkZcp2zJGXIsoYkZcjgLEkZ8toakpQha86SlCFXa0hShiolKWwYnCWVigcEJSlD5cibDc6SSsbMWZIy1BrlyJ0NzpJKpRyh2eAsqWQsa0hShlxKJ0kZKkdoNjhLKhnLGpKUobaS5M4GZ0mlYuYsSRlKZs6SlB8zZ/XK6aedzI03Xkt9XR1zb7ubv/+Hm2o9Je2BNWvX8+XrbmDDpk0EwaemnsmF0855V59fPPIY//uWO6mLOurr67lqZiPHfeg/7tW4m3+3hS9cfT2rX1/LwR8Yx6zrvsQB73tvv4y1ryvLUrpIqX8/SMN+48vxN7UX6urqeOH5RzjjrAtoalrD44/dz/+48PO88MJLtZ5azbSsfqTWU9gj69/YyPoNGznyg0fw9tvNTJtxGd+5/moOP/SQnX2am1sYPnwYEcGLy1/hb67+Jj+9+5Yevf/ip57h3vsX8o2vfuFd7bNu+j4HvO+9fPbCadz6g/n8bssWrvj8jL0aK0dDxhy215fK/9ykaT2OOTe/Oj/bS/PX1XoCg8GUPz6W3/72VV555TW2b9/O/Pn38t/+6+m1npb2wNgxoznyg0cAsP/+IzjskImsXb/hXX1GjBhORPu/+ZatWyF+/+9/7o/+D+fNuIxzL/oc3731Bz0e95ePPMbUM08FYOqZp/KLhx/rdqzBqpXU4y1nljUGwMHjP8DKptU7nzetWsOUPz62hjNSX1i1Zi0vvPRb/tNRH9xt30P/71+Z/b3b2bDpTf7phmsB+NcnnuS1plXMu3U2KSUuvfLrLH36WY4/5uhux9qw6U3GjhkNwJj3j2LDpje7HGswG/QHBCPiMyml2zrZ1wg0AkT9AdTV7b+nw0hZam5u4a+/8rdcednFvGf/3X++T/3YSZz6sZNY+vSzfPeWO7l19vX8eslT/HrxU3zqLy5tf4+WFlasXM3xxxzNBX91Odu2bae5pYXNv9vCn02/BIArPv+XnHTCh9/13hGxM1vubKzBzAOC8HWgw+CcUpoDzAFrzgCrV73OxAkH73w+YfxBrF79eg1npL2xvbWVy7/yt5x92il84uSTuux7/DFH07T6dTa9uRkSfPbC85h2zlm79bv7lm8Dndec3z9qJOvf2MjYMaNZ/8ZGRo88oMuxRnWwf7AoS+bcZc05Ip7pZHsWGDdAc9znLVn6NEcccSiTJk1kyJAhTJs2lZ/e9/NaT0t7IKXE167/NocdMpHp53+ywz6vNa1mx4H2ZS8uZ9u27Yw84H38yZTjuOdffk5zcwsAa9e/8a7yRFdO/tMTufeBhwC494GHOOU/f6TLsQazSi+2nHWXOY8DTgc27dIewK/7ZUYl1NbWxszLv8r9/3IX9XV13H7Hj1m27De1npb2wL898zw//dkiJh8+aWfpYebF01mzdj0A5517Ngt/9SgLHlhEQ0MDw4buxw3XXkVEcNIJH+blFSv584uvAGDE8GFc/7Uv8v5RI7sd97MXTuMLV3+Tn9z3IAd/4EBmXfdlgE7HGsza+nkF2kDpcildRHwfuC2l9GgH++5KKX26uwEsa6gj++pSOvWvvlhK9+lDzu1xzLlrxT1djhcRrwJbgDagNaV0fESMBn4MTAJeBaallDZF+2/F2cBZQDPwFymlp/bkM0A3ZY2U0oyOAnOxr9vALEkDLfXiTw+dklI6JqV0fPH8KmBRSmkysKh4DnAmMLnYGoGb9+ZzuM5ZUqkMQM15KnBH8fgO4Jyq9jtTu8eBkRFx0J4OYnCWVCoVUo+3iGiMiKVVW+Mub5eAn0fEk1X7xqWU1hSPX+f3iyPGAyurXttUtO0RT0KRVCq9WUpXvey3E3+aUloVEQcCCyPi33d5fYron9t9G5wllUpfrtZIKa0qvq6LiHuAKcDaiDgopbSmKFusK7qvAiZWvXxC0bZHLGtIKpXelDW6EhH7R8R7dzwGTgOeAxYA04tu04F7i8cLgIui3YnA5qryR6+ZOUsqlT48uWQccE+xbrwBuCul9LOIWALMj4gZwApgWtH/ftqX0S2nfSndZ/ZmcIOzpFLpq9O3U0ovAx/qoH0D8PEO2hNwSZ8MjsFZUsmU5WL7BmdJpdLfNxAZKAZnSaXSZuYsSfmxrCFJGbKsIUkZMnOWpAyV5U4oBmdJpVKWi+0bnCWVimUNScqQwVmSMuRqDUnKkJmzJGXI1RqSlKG21IcXDa0hg7OkUrHmLEkZsuYsSRmy5ixJGapY1pCk/Jg5S1KGXK0hSRmyrCFJGbKsIUkZMnOWpAyZOUtShtpSW62n0CcMzpJKxdO3JSlDnr4tSRkyc5akDLlaQ5Iy5GoNScqQp29LUoasOUtShqw5S1KGzJwlKUOuc5akDJk5S1KGXK0hSRnygKAkZagsZY26Wk9AkvpS6sWf7kTEGRHxYkQsj4irBmD6O5k5SyqVvsqcI6IeuAn4BNAELImIBSmlZX0yQDcMzpJKpQ9rzlOA5SmllwEiYh4wFShHcG7dtir6e4x9RUQ0ppTm1Hoeyos/F32rNzEnIhqBxqqmOVXfi/HAyqp9TcAJez/DnrHmPLAau++iQcifixpJKc1JKR1ftWXzS9LgLEkdWwVMrHo+oWgbEAZnSerYEmByRBwaEfsB5wMLBmpwDwgOrGz+y6Ss+HORoZRSa0RcCjwI1ANzU0rPD9T4UZYF25JUJpY1JClDBmdJypDBeYDU8jRQ5Ski5kbEuoh4rtZzUX4MzgOg6jTQM4EjgQsi4sjazkoZuB04o9aTUJ4MzgNj52mgKaVtwI7TQDWIpZQeBjbWeh7Kk8F5YHR0Guj4Gs1F0j7A4CxJGTI4D4yangYqad9jcB4YNT0NVNK+x+A8AFJKrcCO00BfAOYP5GmgylNE3A08BnwwIpoiYkat56R8ePq2JGXIzFmSMmRwlqQMGZwlKUMGZ0nKkMFZkjJkcJakDBmcJSlD/x+NJZ6XJafrowAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"rGOA3KRlUViI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626140081152,"user_tz":-480,"elapsed":292,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"87ae0d11-7882-4c37-9790-67192932b42b"},"source":["(cf_matrix[0,1]+cf_matrix[1,0])/np.sum(cf_matrix)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.0002587991718426501"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"xmGI_uJUT9i8"},"source":[""],"execution_count":null,"outputs":[]}]}