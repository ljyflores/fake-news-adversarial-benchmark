{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fakebert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVqe1h6QTeqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71ee1ddc-1fd1-4256-c880-79d88a02517d"
      },
      "source": [
        "from google.colab import drive, files\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4-8mzRfTenc"
      },
      "source": [
        "!pip3 install transformers\n",
        "!cp /content/drive/MyDrive/fake-news-explainability/utils_fake_news.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1qS7W-lTek4"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json, re\n",
        "from tqdm import tqdm_notebook\n",
        "from uuid import uuid4\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import itertools\n",
        "import os\n",
        "\n",
        "## Torch Modules\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import (\n",
        "    Dataset, \n",
        "    DataLoader,\n",
        "    TensorDataset, \n",
        "    random_split, \n",
        "    RandomSampler, \n",
        "    SequentialSampler)\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    RobertaForSequenceClassification,\n",
        "    RobertaTokenizer,\n",
        "    AdamW,\n",
        "    get_linear_schedule_with_warmup)\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "%run utils_fake_news.py\n",
        "\n",
        "# Device\n",
        "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWKDP6yz2DI9"
      },
      "source": [
        "## Model & Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bBYr4RA2CrJ"
      },
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "class FakeBERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FakeBERT, self).__init__()\n",
        "        \n",
        "        self.base_model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "        # Layer 1: Conv1D + Maxpool\n",
        "        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
        "        self.sigm_1 = nn.ReLU()\n",
        "        self.pool_1 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
        "\n",
        "        # Layer 2: Conv1D + Maxpool\n",
        "        self.conv_2 = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=4, stride=1)\n",
        "        self.sigm_2 = nn.ReLU()\n",
        "        self.pool_2 = nn.MaxPool1d(kernel_size=5, stride=5)\n",
        "\n",
        "        # Layer 5: Fully Connected Layer \n",
        "        self.full_5 = nn.Linear(306,128)\n",
        "        self.sigm_5 = nn.Sigmoid()\n",
        "\n",
        "        # Layer 5: Fully Connected Layer \n",
        "        self.full_6 = nn.Linear(128,32)\n",
        "        self.sigm_6 = nn.Sigmoid()\n",
        "        \n",
        "        # Layer 7: Fully Connected Layer \n",
        "        self.full_7 = nn.Linear(32,2)\n",
        "        self.soft_7 = nn.Softmax()\n",
        "\n",
        "    def forward(self, input_ids, attn_mask):\n",
        "        bert_output = self.base_model(input_ids, attention_mask=attn_mask)\n",
        "        bert_output = bert_output['pooler_output'].unsqueeze(1)\n",
        "\n",
        "        output1 = self.pool_1(self.sigm_1(self.conv_1(bert_output))).squeeze(1)\n",
        "        output2 = self.pool_2(self.sigm_2(self.conv_2(bert_output))).squeeze(1)\n",
        "        outputs = torch.cat((output1,output2),dim=1)\n",
        "        outputs = self.sigm_5(self.full_5(outputs))\n",
        "        outputs = self.sigm_6(self.full_6(outputs))\n",
        "        outputs = self.soft_7(self.full_7(outputs))\n",
        "        return outputs\n",
        "\n",
        "def train():\n",
        "    total_t0 = time.time()\n",
        "    for epoch_i in range(0, epochs):\n",
        "        \n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        t0 = time.time()\n",
        "        total_train_loss = 0\n",
        "        bert_model.train()\n",
        "\n",
        "        for step, batch in enumerate(bert_train_dataloader):\n",
        "\n",
        "            # Unpack batch\n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Zero grads\n",
        "            bert_model.zero_grad()        \n",
        "\n",
        "            # Forward pass\n",
        "            output = bert_model(b_input_ids, b_input_mask)\n",
        "            \n",
        "            # Accumulate loss\n",
        "            loss = loss_func(output.squeeze(1), b_labels)\n",
        "\n",
        "            # Backpropagate\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The bert_optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            bert_optimizer.step()\n",
        "\n",
        "            # Update the learning rate.\n",
        "            bert_scheduler.step()\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n",
        "                print(loss.detach())\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(bert_train_dataloader)            \n",
        "        \n",
        "        # Measure how long this epoch took.\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        bert_training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                # 'Valid. Loss': avg_val_loss,\n",
        "                # 'Valid. Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                # 'Validation Time': validation_time\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFptuf6I7oFC"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYWfkdPw7n52"
      },
      "source": [
        "# Load encoded Fake-News Kaggle dataset\n",
        "df_train_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/training/fake_news.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rn4E6EnF7nuY"
      },
      "source": [
        "# Load data into dataloader\n",
        "batch_size = 32\n",
        "bert_train_dataloader = DataLoader(\n",
        "    df_train_encode,  # The training samples.\n",
        "    batch_size = batch_size # Trains with this batch size.\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a-6wuweTwDo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTA3a85IDGYM"
      },
      "source": [
        "# BERT\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Model\n",
        "bert_model = FakeBERT().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI21IIB26NGl"
      },
      "source": [
        "# Optimizer\n",
        "bert_optimizer = AdamW(bert_model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "# Training Params\n",
        "bert_training_stats = []\n",
        "epochs = 2\n",
        "total_steps = len(bert_train_dataloader) * epochs\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "# Learning rate scheduler.\n",
        "bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n",
        "                                                 num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                 num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrEft0thUO03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dbba648-18e6-43cd-892b-4cbdecba4bf6"
      },
      "source": [
        "# Train or load pre-trained\n",
        "bert_model_path = \"/content/drive/MyDrive/fake-news-explainability/Models/fake_news_model\"\n",
        "\n",
        "if os.path.exists(bert_model_path):\n",
        "    bert_model = FakeBERT().to(device)\n",
        "    bert_model.load_state_dict(torch.load(bert_model_path))\n",
        "else:\n",
        "    train()\n",
        "    torch.save(bert_model.state_dict(), bert_model_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nqa-BzdNUDrY"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FANxOr7uSapT",
        "outputId": "8cceb6bf-1b4d-48d8-ce86-b4057a974761"
      },
      "source": [
        "# Load encoded tensors\n",
        "df_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/training/fake_news.pt\")\n",
        "\n",
        "# Load data into dataloader\n",
        "batch_size = 32\n",
        "bert_dataloader = DataLoader(df_encode, batch_size = batch_size)\n",
        "\n",
        "# Generate predictions\n",
        "outputs = []\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(bert_dataloader):\n",
        "        # Unpack batch\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output = bert_model(b_input_ids, b_input_mask)\n",
        "        outputs.append(output)        \n",
        "        \n",
        "# Stack outputs\n",
        "outputs = torch.vstack(outputs)\n",
        "\n",
        "# Accuracy\n",
        "print(f\"Acc: {flat_accuracy(outputs.detach().cpu().numpy(), df_encode.tensors[2].to('cpu').numpy())}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Acc: 0.9101373382076869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS-V0-jfHaIq"
      },
      "source": [
        "## Adversarial Attack Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbnZFGfQ80Au"
      },
      "source": [
        "# Load encoded tensors\n",
        "# df_pos_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_pos.pt\")\n",
        "# df_neg_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_neg.pt\")\n",
        "\n",
        "df_pos_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_name_orig_filtered.pt\")\n",
        "df_neg_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_name_new_filtered.pt\")\n",
        "\n",
        "# df_pos_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_polarity_orig_filtered.pt\")\n",
        "# df_neg_encode = torch.load(\"/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_polarity_new_filtered.pt\")\n",
        "\n",
        "# Load data into dataloader\n",
        "batch_size = 32\n",
        "bert_pos_dataloader = DataLoader(df_pos_encode, batch_size = batch_size)\n",
        "bert_neg_dataloader = DataLoader(df_neg_encode, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJoDmN0N5VYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b824b5c1-f4b3-493b-c209-fb5898457b6e"
      },
      "source": [
        "# Generate predictions\n",
        "# Can't input everything directly due to RAM issues\n",
        "outputs_pos = []\n",
        "outputs_neg = []\n",
        "with torch.no_grad():\n",
        "    for step, batch in enumerate(bert_pos_dataloader):\n",
        "        # Unpack batch\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output = bert_model(b_input_ids, b_input_mask)\n",
        "        outputs_pos.append(output)\n",
        "        \n",
        "    for step, batch in enumerate(bert_neg_dataloader):\n",
        "        # Unpack batch\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        output = bert_model(b_input_ids, b_input_mask)\n",
        "        outputs_neg.append(output)\n",
        "        \n",
        "# Stack outputs\n",
        "outputs_pos = torch.vstack(outputs_pos)\n",
        "outputs_neg = torch.vstack(outputs_neg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cyrd7N798OFR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c96542c-590f-4fc1-a66a-ed86f86c01a8"
      },
      "source": [
        "# Percent Labels Flipped\n",
        "cf_matrix = confusion_matrix(torch.argmax(outputs_pos.cpu(), axis=1), \n",
        "                             torch.argmax(outputs_neg.cpu(), axis=1))\n",
        "print(f\"Labels Flipped: {cf_matrix[0,1]+cf_matrix[1,0]} of {np.sum(cf_matrix)} ({round(100*(cf_matrix[0,1]+cf_matrix[1,0])/np.sum(cf_matrix),4)}%)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels Flipped: 1 of 248 (0.4032%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhwuc3mx8NoC",
        "outputId": "852daff6-27a2-4bc1-a6e2-4dcc8e3333e8"
      },
      "source": [
        "# Average Probability Change\n",
        "m = nn.Softmax(dim=1)\n",
        "delta_lst = m(outputs_neg)[:,1]-m(outputs_pos)[:,1]\n",
        "print(f\"Average Change: {round(float(torch.mean(delta_lst)),4)} ({round(float(torch.std(delta_lst)),4)})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Change: 0.0015 (0.0234)\n"
          ]
        }
      ]
    }
  ]
}