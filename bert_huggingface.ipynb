{"nbformat":4,"nbformat_minor":5,"metadata":{"environment":{"name":"pytorch-gpu.1-7.m65","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"colab":{"name":"bert_huggingface.ipynb","provenance":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEXrlTZzTONe","executionInfo":{"status":"ok","timestamp":1621681136478,"user_tz":-480,"elapsed":759,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"fc43185e-2f91-42ad-833d-d82a74f26ecd"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"cEXrlTZzTONe","execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IzUQGNmpS-8F"},"source":["!pip3 install transformers"],"id":"IzUQGNmpS-8F","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clean-insulation","executionInfo":{"status":"ok","timestamp":1621681152635,"user_tz":-480,"elapsed":7531,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["import pandas as pd\n","import numpy as np\n","import json, re\n","from tqdm import tqdm_notebook\n","from uuid import uuid4\n","import time\n","import datetime\n","import random\n","\n","## Torch Modules\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# Transformers\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import (\n","    BertForSequenceClassification,\n","                          BertTokenizer,\n","                          RobertaForSequenceClassification,\n","                          RobertaTokenizer,\n","                         AdamW)"],"id":"clean-insulation","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"streaming-continuity","executionInfo":{"status":"ok","timestamp":1621681152644,"user_tz":-480,"elapsed":17,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["# Target labels\n","label_encodings6 = {\n","    'pants-fire': 0, \n","    'false':      1, \n","    'barely-true':2, \n","    'half-true':  3, \n","    'mostly-true':4,\n","    'true':       5\n","}"],"id":"streaming-continuity","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"verified-duplicate","executionInfo":{"status":"ok","timestamp":1621681152644,"user_tz":-480,"elapsed":15,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["def encode_dataframe(statement_col, target_col, unpack=False):\n","    # Tokenize statements\n","    bert_encoded_dict = statement_col.apply(lambda sent: bert_tokenizer.encode_plus(\n","                                      sent,                      # Sentence to encode.\n","                                      add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                                      max_length = 120,           # Pad & truncate all sentences.\n","                                      pad_to_max_length = True,\n","                                      return_attention_mask = True,   # Construct attn. masks.\n","                                      return_tensors = 'pt',     # Return pytorch tensors.\n","                                      truncation = True\n","                                ))\n","    bert_input_ids = torch.cat([item['input_ids'] for item in bert_encoded_dict], dim=0)\n","    bert_attention_masks = torch.cat([item['attention_mask'] for item in bert_encoded_dict], dim=0)\n","\n","    # Format targets\n","    labels = torch.tensor(target_col)\n","    sentence_ids = torch.tensor(range(len(target_col)))\n","\n","    # Combine the training inputs into a TensorDataset\n","    bert_dataset = TensorDataset(sentence_ids, bert_input_ids, bert_attention_masks, labels)\n","\n","    # Remove indices\n","    trial_dataset =  index_remover(bert_dataset)\n","\n","    if unpack:\n","        return bert_input_ids, bert_attention_masks, labels\n","    else:\n","        return trial_dataset\n","\n","def index_remover(tensordata):\n","    input_ids = []\n","    attention_masks = []\n","    labels = []\n","   \n","    for a,b,c,d in tensordata:\n","        input_ids.append(b.tolist())\n","        attention_masks.append(c.tolist())\n","        labels.append(d.tolist())\n","        \n","    input_ids = torch.tensor(input_ids)\n","    attention_masks = torch.tensor(attention_masks)\n","    labels = torch.tensor(labels)\n","    \n","    final_dataset =  TensorDataset(input_ids, attention_masks, labels)\n","    return final_dataset\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"id":"verified-duplicate","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"disturbed-ivory","executionInfo":{"status":"ok","timestamp":1621681177131,"user_tz":-480,"elapsed":24499,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"35cf2f6c-aaa7-48a0-d8b1-236844060067"},"source":["# Device\n","device = torch.device(\"cuda:0\")\n","\n","# BERT\n","bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n","                                                           num_labels = 6, # The number of output labels--2 for binary classification.\n","                                                                           # You can increase this for multi-class tasks.\n","                                                           output_attentions = False, # Whether the model returns attentions weights.\n","                                                           output_hidden_states = False # Whether the model returns all hidden-states.\n","                                                          ).to(device)\n","bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"],"id":"disturbed-ivory","execution_count":6,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"confidential-morgan","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621681184495,"user_tz":-480,"elapsed":7372,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"62f54019-8746-4b23-bc99-8f3ccdb64929"},"source":["# Read in data\n","df_train = pd.read_csv(\"/content/drive/MyDrive/fake-news-explainability/train.tsv\", sep='\\t', header=None)\n","df_test = pd.read_csv(\"/content/drive/MyDrive/fake-news-explainability/test.tsv\", sep='\\t', header=None)\n","df_valid = pd.read_csv(\"/content/drive/MyDrive/fake-news-explainability/valid.tsv\", sep='\\t', header=None)\n","\n","# Relabel columns\n","cols = ['ID', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state',\n","       'party', 'barely_true_count', 'false_count', 'half_true_count',\n","       'mostly_true_count', 'pants_on_fire_count', 'context']\n","df_train.columns, df_test.columns, df_valid.columns = cols, cols, cols\n","\n","# Relabel target\n","df_train['target'] = df_train['label'].apply(lambda x: label_encodings6[x])\n","df_test['target'] = df_test['label'].apply(lambda x: label_encodings6[x])\n","df_valid['target'] = df_valid['label'].apply(lambda x: label_encodings6[x])\n","\n","# Encode dataframes\n","df_train_encode = encode_dataframe(df_train['statement'], df_train['target'])\n","df_test_encode = encode_dataframe(df_test['statement'], df_test['target'])"],"id":"confidential-morgan","execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"polyphonic-concrete","executionInfo":{"status":"ok","timestamp":1621681184496,"user_tz":-480,"elapsed":27,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["# Load data into dataloader\n","batch_size = 32\n","\n","bert_train_dataloader = DataLoader(\n","            df_train_encode,  # The training samples.\n","            sampler = RandomSampler(df_train_encode), # Select batches randomly\n","            batch_size = batch_size # Trains with this batch size.\n","        )\n","bert_validation_dataloader = DataLoader(\n","            df_test_encode, # The validation samples.\n","            sampler = SequentialSampler(df_test_encode), # Pull out batches sequentially.\n","            batch_size = batch_size # Evaluate with this batch size.\n","        )"],"id":"polyphonic-concrete","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"available-coffee","executionInfo":{"status":"ok","timestamp":1621681184497,"user_tz":-480,"elapsed":25,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["# Optimizer\n","bert_optimizer = AdamW(bert_model.parameters(),\n","                  lr = 5e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n","                )\n","\n","# Epochs & Learning Rate\n","epochs = 2\n","total_steps = len(bert_train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"],"id":"available-coffee","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peaceful-fashion","executionInfo":{"status":"ok","timestamp":1621681622996,"user_tz":-480,"elapsed":438523,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"9e5f5cbc-d0a5-4f5d-fc07-67a140a47c0c"},"source":["# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 100\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","bert_training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    \n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    # Put the bert_model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-bert_model-train-do-in-pytorch)\n","    bert_model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(bert_train_dataloader):\n","\n","        # Progress update every 40 batches.\n","        if step % 40 == 0 and not step == 0:\n","            elapsed = format_time(time.time() - t0)\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n","\n","        # Unpack batch\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Zero grads\n","        bert_model.zero_grad()        \n","\n","        # Forward pass\n","        output = bert_model(b_input_ids, \n","                            token_type_ids=None, \n","                             attention_mask=b_input_mask, \n","                             labels=b_labels)\n","        # Accumulate loss\n","        total_train_loss += output[0].item()\n","\n","        # Backward pass\n","        output[0].backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The bert_optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        bert_optimizer.step()\n","\n","        # Update the learning rate.\n","        bert_scheduler.step()\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(bert_train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the bert_model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    bert_model.eval()\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in bert_validation_dataloader:\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        with torch.no_grad():        \n","            outputs = bert_model(b_input_ids, \n","                                   token_type_ids=None, \n","                                   attention_mask=b_input_mask,\n","                                   labels=b_labels)\n","            \n","        total_eval_loss += outputs[0].item()\n","\n","        # Move logits and labels to CPU\n","        logits = outputs[1].detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(bert_validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(bert_validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    bert_training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"id":"peaceful-fashion","execution_count":10,"outputs":[{"output_type":"stream","text":["======== Epoch 1 / 2 ========\n","Training...\n","  Batch    40  of    320.    Elapsed: 0:00:24.\n","  Batch    80  of    320.    Elapsed: 0:00:50.\n","  Batch   120  of    320.    Elapsed: 0:01:17.\n","  Batch   160  of    320.    Elapsed: 0:01:43.\n","  Batch   200  of    320.    Elapsed: 0:02:09.\n","  Batch   240  of    320.    Elapsed: 0:02:36.\n","  Batch   280  of    320.    Elapsed: 0:03:02.\n","\n","  Average training loss: 1.72\n","  Training epoch took: 0:03:28\n","\n","Running Validation...\n","  Accuracy: 0.27\n","  Validation Loss: 1.67\n","  Validation took: 0:00:10\n","======== Epoch 2 / 2 ========\n","Training...\n","  Batch    40  of    320.    Elapsed: 0:00:26.\n","  Batch    80  of    320.    Elapsed: 0:00:53.\n","  Batch   120  of    320.    Elapsed: 0:01:19.\n","  Batch   160  of    320.    Elapsed: 0:01:45.\n","  Batch   200  of    320.    Elapsed: 0:02:12.\n","  Batch   240  of    320.    Elapsed: 0:02:38.\n","  Batch   280  of    320.    Elapsed: 0:03:04.\n","\n","  Average training loss: 1.58\n","  Training epoch took: 0:03:31\n","\n","Running Validation...\n","  Accuracy: 0.29\n","  Validation Loss: 1.66\n","  Validation took: 0:00:10\n","\n","Training complete!\n","Total training took 0:07:19 (h:mm:ss)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"competent-klein"},"source":["### Validation & Reporting Accuracy"],"id":"competent-klein"},{"cell_type":"code","metadata":{"id":"sharing-david","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621681647874,"user_tz":-480,"elapsed":24889,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}},"outputId":"dea2dcfd-a932-4ee6-e91f-1f36f96a0774"},"source":["# Reformat test and validation dataframes\n","input_test,  attention_test,  labels_test  = encode_dataframe(df_test['statement'],  df_test['target'],  unpack=True)\n","input_valid, attention_valid, labels_valid = encode_dataframe(df_valid['statement'], df_valid['target'], unpack=True)\n","\n","# Run the inputs through the model\n","with torch.no_grad():\n","    outputs_test = bert_model(input_test.to(device),\n","                              token_type_ids=None, \n","                              attention_mask=attention_test.to(device),\n","                              labels=labels_test.to(device))\n","    outputs_valid = bert_model(input_valid.to(device),\n","                               token_type_ids=None, \n","                               attention_mask=attention_valid.to(device),\n","                               labels=labels_valid.to(device))\n","    \n","# Test accuracy\n","print(f\"Test Acc: {flat_accuracy(outputs_test[1].detach().cpu().numpy(), labels_test.to('cpu').numpy())}\")\n","\n","# Valid accuracy\n","print(f\"Valid Acc: {flat_accuracy(outputs_valid[1].detach().cpu().numpy(), labels_valid.to('cpu').numpy())}\")"],"id":"sharing-david","execution_count":11,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Test Acc: 0.2943962115232833\n","Valid Acc: 0.2757009345794392\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IX7X-Tugu6hT","executionInfo":{"status":"ok","timestamp":1621681683518,"user_tz":-480,"elapsed":1830,"user":{"displayName":"Lj Flores","photoUrl":"","userId":"11095487892395270199"}}},"source":["# Save model\n","bert_model.save_pretrained(\"/content/drive/MyDrive/fake-news-explainability/bert_model\")"],"id":"IX7X-Tugu6hT","execution_count":12,"outputs":[]}]}