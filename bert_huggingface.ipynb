{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clean-insulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, re\n",
    "from tqdm import tqdm_notebook\n",
    "from uuid import uuid4\n",
    "\n",
    "## Torch Modules\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "focused-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriental-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pre-trained models\n",
    "from transformers import (\n",
    "    BertForSequenceClassification,\n",
    "                          BertTokenizer,\n",
    "                          RobertaForSequenceClassification,\n",
    "                          RobertaTokenizer,\n",
    "                         AdamW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "streaming-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target labels\n",
    "label_encodings3 = {\n",
    "    'pants-fire': 0, \n",
    "    'false':      0, \n",
    "    'barely-true':1, \n",
    "    'half-true':  1, \n",
    "    'mostly-true':2,\n",
    "    'true':       2\n",
    "}\n",
    "label_encodings6 = {\n",
    "    'pants-fire': 0, \n",
    "    'false':      1, \n",
    "    'barely-true':2, \n",
    "    'half-true':  3, \n",
    "    'mostly-true':4,\n",
    "    'true':       5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "verified-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_remover(tensordata):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "   \n",
    "    for a,b,c,d in tensordata:\n",
    "        input_ids.append(b.tolist())\n",
    "        attention_masks.append(c.tolist())\n",
    "        labels.append(d.tolist())\n",
    "        \n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    final_dataset =  TensorDataset(input_ids, attention_masks, labels)\n",
    "    return final_dataset\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "disturbed-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# BERT\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "                                                           num_labels = 6, # The number of output labels--2 for binary classification.\n",
    "                                                                           # You can increase this for multi-class tasks.\n",
    "                                                           output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                                           output_hidden_states = False # Whether the model returns all hidden-states.\n",
    "                                                          )\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "confidential-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "\n",
    "df_train = pd.read_csv(\"Data/liar_dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"Data/liar_dataset/test.csv\")\n",
    "df_valid = pd.read_csv(\"Data/liar_dataset/valid.csv\")\n",
    "\n",
    "# df_train = pd.read_csv(\"Data/liar_dataset/aug/train_aug.tsv\", sep='\\t', header=None).drop(0, axis=1)\n",
    "# df_test = pd.read_csv(\"Data/liar_dataset/aug/test_aug.tsv\", sep='\\t', header=None).drop(0, axis=1)\n",
    "# df_valid = pd.read_csv(\"Data/liar_dataset/aug/valid_aug.tsv\", sep='\\t', header=None).drop(0, axis=1)\n",
    "\n",
    "# # Relabel columns\n",
    "# cols = ['ID', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state',\n",
    "#        'party', 'barely_true_count', 'false_count', 'half_true_count',\n",
    "#        'mostly_true_count', 'pants_on_fire_count', 'context']\n",
    "\n",
    "# df_train.columns = cols\n",
    "# df_test.columns = cols\n",
    "# df_valid.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "angry-fleece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Relabel\n",
    "df_train['target'] = df_train['label'].apply(lambda x: label_encodings6[x])\n",
    "df_test['target'] = df_test['label'].apply(lambda x: label_encodings6[x])\n",
    "df_valid['target'] = df_valid['label'].apply(lambda x: label_encodings6[x])\n",
    "\n",
    "# Generate indexed tokens per statement\n",
    "bert_input_ids = []\n",
    "bert_attention_masks = []\n",
    "\n",
    "for sent in df_train['statement']:\n",
    "    bert_encoded_dict = bert_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 120,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation = True\n",
    "                   )\n",
    "    bert_input_ids.append(bert_encoded_dict['input_ids'])\n",
    "    bert_attention_masks.append(bert_encoded_dict['attention_mask'])\n",
    "    \n",
    "# Convert the lists into tensors.\n",
    "bert_input_ids = torch.cat(bert_input_ids, dim=0)\n",
    "bert_attention_masks = torch.cat(bert_attention_masks, dim=0)\n",
    "\n",
    "# Get targets\n",
    "labels = torch.tensor(df_train['target'])\n",
    "sentence_ids = torch.tensor(range(df_train.shape[0]))\n",
    "\n",
    "# function to seed the script globally\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "bert_dataset = TensorDataset(sentence_ids, bert_input_ids, bert_attention_masks, labels)\n",
    "\n",
    "# Remove indices\n",
    "trial_dataset =  index_remover(bert_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "statutory-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "\n",
    "# Calculate the number of samples to include in each set.\n",
    "train_size = int(0.9 * len(bert_dataset))\n",
    "val_size = len(bert_dataset) - train_size\n",
    "\n",
    "# Divide the dataset by randomly selecting samples.\n",
    "bert_train_dataset, bert_val_dataset = random_split(bert_dataset, [train_size, val_size])\n",
    "\n",
    "# removing sentence ids from tensor dataset so that it can be used for training \n",
    "bert_train_dataset = index_remover(bert_train_dataset)\n",
    "bert_val_dataset = index_remover(bert_val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "polyphonic-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoaders for our training and validation sets.\n",
    "# We'll take training samples in random order. \n",
    "bert_train_dataloader = DataLoader(\n",
    "            bert_train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(bert_train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "bert_validation_dataloader = DataLoader(\n",
    "            bert_val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(bert_val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "available-coffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "bert_optimizer = AdamW(bert_model.parameters(),\n",
    "                  lr = 5e-5, # args.learning_rate - default is 5e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "# Epochs & Learning Rate\n",
    "epochs = 2\n",
    "total_steps = len(bert_train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "bert_scheduler = get_linear_schedule_with_warmup(bert_optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "second-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "peaceful-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    288.    Elapsed: 0:05:39.\n",
      "  Batch    80  of    288.    Elapsed: 0:11:16.\n",
      "  Batch   120  of    288.    Elapsed: 0:16:50.\n",
      "  Batch   160  of    288.    Elapsed: 0:22:36.\n",
      "  Batch   200  of    288.    Elapsed: 0:28:11.\n",
      "  Batch   240  of    288.    Elapsed: 0:33:52.\n",
      "  Batch   280  of    288.    Elapsed: 0:39:43.\n",
      "\n",
      "  Average training loss: 1.73\n",
      "  Training epcoh took: 0:41:06\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.29\n",
      "  Validation Loss: 1.67\n",
      "  Validation took: 0:01:42\n",
      "======== Epoch 2 / 2 ========\n",
      "Training...\n",
      "  Batch    40  of    288.    Elapsed: 0:06:52.\n",
      "  Batch    80  of    288.    Elapsed: 0:13:45.\n",
      "  Batch   120  of    288.    Elapsed: 0:20:36.\n",
      "  Batch   160  of    288.    Elapsed: 0:27:27.\n",
      "  Batch   200  of    288.    Elapsed: 0:34:18.\n",
      "  Batch   240  of    288.    Elapsed: 0:41:21.\n",
      "  Batch   280  of    288.    Elapsed: 0:47:52.\n",
      "\n",
      "  Average training loss: 1.58\n",
      "  Training epcoh took: 0:49:00\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.28\n",
      "  Validation Loss: 1.66\n",
      "  Validation took: 0:01:29\n",
      "\n",
      "Training complete!\n",
      "Total training took 1:33:17 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# This training code is based on the `run_glue.py` script here:\n",
    "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 100\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# We'll store a number of quantities such as training and validation loss, \n",
    "# validation accuracy, and timings.\n",
    "bert_training_stats = []\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # Put the bert_model into training mode. Don't be mislead--the call to \n",
    "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "    # `dropout` and `batchnorm` layers behave differently during training\n",
    "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-bert_model-train-do-in-pytorch)\n",
    "    bert_model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(bert_train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(bert_train_dataloader), elapsed))\n",
    "\n",
    "        # Unpack batch\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        # Zero grads\n",
    "        bert_model.zero_grad()        \n",
    "\n",
    "        # Forward pass\n",
    "        output = bert_model(b_input_ids, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=b_input_mask, \n",
    "                             labels=b_labels)\n",
    "        # Accumulate loss\n",
    "        total_train_loss += output[0].item()\n",
    "\n",
    "        # Backward pass\n",
    "        output[0].backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(bert_model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        # The bert_optimizer dictates the \"update rule\"--how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        bert_optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        bert_scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(bert_train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the bert_model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    bert_model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in bert_validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        \n",
    "        with torch.no_grad():        \n",
    "            outputs = bert_model(b_input_ids, \n",
    "                                   token_type_ids=None, \n",
    "                                   attention_mask=b_input_mask,\n",
    "                                   labels=b_labels)\n",
    "            \n",
    "        total_eval_loss += outputs[0].item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences, and\n",
    "        # accumulate it over all batches.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(bert_validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(bert_validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    bert_training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-klein",
   "metadata": {},
   "source": [
    "### Validation & Reporting Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "brutal-portland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the test set\n",
    "bert_input_ids_test = []\n",
    "bert_attention_masks_test = []\n",
    "\n",
    "for sent in df_test['statement']:\n",
    "    bert_encoded_dict = bert_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 120,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation = True\n",
    "                   )\n",
    "    bert_input_ids_test.append(bert_encoded_dict['input_ids'])\n",
    "    bert_attention_masks_test.append(bert_encoded_dict['attention_mask'])\n",
    "    \n",
    "# Convert the lists into tensors.\n",
    "bert_input_ids_test = torch.cat(bert_input_ids_test, dim=0)\n",
    "bert_attention_masks_test = torch.cat(bert_attention_masks_test, dim=0)\n",
    "\n",
    "# Get labels\n",
    "bert_labels_test = torch.tensor(df_test['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "perfect-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the validation set\n",
    "bert_input_ids_valid = []\n",
    "bert_attention_masks_valid = []\n",
    "\n",
    "for sent in df_valid['statement']:\n",
    "    bert_encoded_dict = bert_tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 120,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation = True\n",
    "                   )\n",
    "    bert_input_ids_valid.append(bert_encoded_dict['input_ids'])\n",
    "    bert_attention_masks_valid.append(bert_encoded_dict['attention_mask'])\n",
    "    \n",
    "# Convert the lists into tensors.\n",
    "bert_input_ids_valid = torch.cat(bert_input_ids_valid, dim=0)\n",
    "bert_attention_masks_valid = torch.cat(bert_attention_masks_valid, dim=0)\n",
    "\n",
    "# Get labels\n",
    "bert_labels_valid = torch.tensor(df_valid['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dominican-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inputs through the model\n",
    "with torch.no_grad():\n",
    "    outputs_test = bert_model(bert_input_ids_test,\n",
    "                              token_type_ids=None, \n",
    "                              attention_mask=bert_attention_masks_test,\n",
    "                              labels=bert_labels_test)\n",
    "    outputs_valid = bert_model(bert_input_ids_valid,\n",
    "                               token_type_ids=None, \n",
    "                               attention_mask=bert_attention_masks_valid,\n",
    "                               labels=bert_labels_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "configured-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Dataset, Test & Validation Reporting Functions\n",
    "def get_first_idx(lst):\n",
    "    latest_idx = [0]\n",
    "    latest = lst[0]\n",
    "    for i in range(1,len(lst)):\n",
    "        if lst[i]!=latest:\n",
    "            latest_idx.append(i)\n",
    "            latest = lst[i]\n",
    "    return latest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "sharing-david",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2833464877663773"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([a==b for a,b in zip(np.argmax(outputs_test[1].detach().cpu().numpy()[get_first_idx(df_test['ID'])], axis=1), \n",
    "                             bert_labels_test.detach().numpy()[get_first_idx(df_test['ID'])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "disturbed-installation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26713395638629284"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([a==b for a,b in zip(np.argmax(outputs_valid[1].detach().cpu().numpy()[get_first_idx(df_valid['ID'])], axis=1), \n",
    "                             bert_labels_valid.detach().numpy()[get_first_idx(df_valid['ID'])])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "general-deployment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 22  23   7   4   2   3]\n",
      " [ 46  76  40  39  32  15]\n",
      " [  8  27  30  24  17   6]\n",
      " [ 27  74 101 111  78  53]\n",
      " [  5  32  41  43  67  55]\n",
      " [  8  31  18  27  55  37]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA2W0lEQVR4nO3deVzU1f7H8deZGWRXJEVIvbmmqaW5L2nmlkuplS3XSq/ZtVzSzMrMtLQyy8wWKzOXFsslyzWzvBq55k5qoolpLiCIgggowsz5/TGEkgqDDp6Z+X2e9/F9MPP9Dt/vWy59OHO+Z85RWmuEEEJcexbTAYQQ4v8rKcBCCGGIFGAhhDBECrAQQhgiBVgIIQyxFfcFOlTs6FXDLGzK+/4m/Zqyz3SEIsvMzjIdoUhyHHbTEYrMopTpCEV2LuvIVYfOTv7T5ZrjV6aK0R+S91UbIYTwEcXeAhZCiGvKi96tSAEWQvgWe47pBC6TAiyE8ClaO0xHcJkUYCGEb3FIARZCCDOkBSyEEIbITTghhDBEWsBCCGGGllEQQghhiNyEE0IIQ6QLQgghDJGbcEIIYYi0gN2rbFQZnnv3OcLLlsbh0Cz7ehkLZyzivyMfp2m7JmRn55DwVzxvD3uHjLQM03EBKBNVhmGThlG6bGm01iz/ejmLZizi0WGP0rRDUxwOB6dOnOKdYe9wMvGk6bj5VKtememfvZf3vFKlirzx+ntM+egzc6FcZLFYWL9+KfHxidx7bx/TcQpUocL1fDbjPcpFlsXhcDBt2ld8MHm66ViX5e/vz6qV3+LvXwKbzcp33y1j7KsTTce6mBfdhFPFvSinO6ajDI8IJzwinLhdcQQGB/Lhsg945fGxlI0qw/Z1MTjsDvqOeAyA6W/MuKpruWs6ytIRpQmPCGf/rv0EBgfy/vfvM/a/Y0lOSOZM+hkAuvbpyr+q/4vJL06+qmsV53SUFouF3/9YS/s7enDkcLzbzltc01EOHvw4DRrcQmhoqFsLcHFMRxkZGUFUZATbY3YREhLMpo3Lua/HY8TGuuf/z+KYjjI4OIiMjExsNhvRPy/gmWEvs2nTNred3x3TUWbt+NHlmuN/y50yHWVhTiadJG5XHABnMs5wKO4wZSKvY+vqbTjszrcbe7bvoWxUGZMx80lJSmH/rv3A35kPUSayTF7xBQgICsDTV6W+vXVzDh445NbiW1zKl4+kU6e2zJw5x3QUlxw7lsT2mF0ApKdnsGfPPspfH2k4VcEyMjIB8POz4edn88jfX63tLm+mFdoFoZSqCXQDygMaiAcWa61jiznbJZWrUI5qtauyZ/vefPvvfKADvyxZbSJSoSIqRFC1dlX2bN8DQK/netH2vrZknM7ghQdfMJyuYPf26MK33yw1HcMlEya8wosvjiM0NNh0lCK74YYK1Ktbh42btpuOUiCLxcLGX3+gatVKTJnyOZs3e2BeL+oDLrAFrJQaDswBFLAJ2Jz7eLZS6rKVQynVTym1RSm15Uj6YbeFDQgKYPQnL/HxK5+QmZ6Zt//fTz2E3W5n5YJVbruWuwQEBTDyk5FMHTM1r/X7xYQv6N20N9ELo7n7P3cbTnh5fn5+dOzchkULfjAdpVCdOrXl+PFktm/faTpKkQUHBzFv7qc88+zLnD6dbjpOgRwOB40a30nlKo1o2LAetWvVMB3pYg6H65thhXVB9AUaaa3Ha61n5W7jgca5xy5Jaz1Va91Qa92wQkhFtwS12qyMnjqKVQt/Zt3ydXn72/doR5O2TRj/1FtuuY47WW1WRn4ykugF0axfvv6i49ELo2nRqYWBZK5p16EVO2J2c/z4CdNRCtW8eUO6dGnP3r3r+OKLybRu3ZyZM981HatQNpuNb+Z+yuzZC1i40PP/0P3t1Kk0Vq/eQIc7W5uOcjHtcH0rhFJqhlIqSSm164J94UqpFUqpfblfS19wbIRSKk4ptVcpdWdh5y+sADuA6y+xPyr32DXzzIShHNp3iG8//S5vX8PWDXig//28/NgrZJ31vDXGnp7wNIfjDrNg2oK8fddXOv/jbNK+CUf2HzERzSX39biLb+d7R/fDqFFvUq1aE2rUaEGvXoOIjl5Pnz5Pm45VqE+nTiR2TxzvvjfVdJRClSkTTqlSJQEICAigTZvb2Ls3znCqS7Bnu74V7jOg4z/2vQCs1FpXB1bmPkcpVQt4CKid+z0fKaWsBZ28sD7gp4GVSql9wN99Cf8CqgGDXEnvDrUb1aZ9j3b8GXuAj5d/CMCMNz9jwNj+lCjhx/ivxwEQu20P77/4wbWKVaBajWrR9r62HIg9wAc/ODN9/tbn3PngnZSvWh7t0CQdTWLyiKsbAVFcAgMDaN2mBUOHjDIdxWe1aN6IRx/pwY6du9my+ScARo0azw/LPa8rDSAqshzTp0/CarVisSjmz1/KsmUrTce6mBu7FrTWq5VSlf6xuxvQOvfx50A0MDx3/xytdRZwQCkVh7O3YMPlzl/oMDSllCX3JOVx9v8eATZrF28hyqrIxU9WRS5+siryteGOYWhnN8x2ueYENu/5BNDvgl1Ttdb53o7kFuClWus6uc9TtdZhFxxP0VqXVkpNBn7VWs/K3T8d+EFrPf9y1y90FIR2ru/xq6v/ICGEMKoILeDcYuuu/p9L/fEo8I+BV3wSTgghXFb8oxsSlVJRWusEpVQUkJS7/whw4aiDCjiH7V6W973fFkKIAmh7tsvbFVoM9M593BtYdMH+h5RS/kqpykB1nMN3L0tawEII3+LGD2IopWbjvOFWRil1BHgZGA/MU0r1BQ4B9wNorX9XSs0DdgM5wMDC7pVJARZC+Bb3joL492UOtb3M618HXnf1/FKAhRC+xYs+iiwFWAjhWzzgI8aukgIshPAt0gIWQghDcrxnQnYpwEII3yItYCGEMET6gIUQwhBpAQshhCHSAj6vl91z1mlzRde7j5uOUGRTf2hiOkKRZXvZRF1H1RV/bNWY1WcPmY5ghrSAhRDCEBkFIYQQhnjgSs2XIwVYCOFbpA9YCCEMkQIshBCGyE04IYQwxO496/dJARZC+BbpghBCCEOkAAshhCHSByyEEGZoh4wDFkIIM6QLQgghDJFREEIIYYi0gIUQwhApwMVDWRSdlr9KZkIK0b0nAlDjsfbU6NMBR46doytj2P7aHMMpnSzlKhD43xfPPy8TSdaSLzm3cgF+d3SlROuu4HCQs3MjWd9NN5g0P2VRPLL0VdITU1jQZyIBpYK566NBlKpQllNHjrNkwAdknco0HRMAq78fj8x7CWsJGxablb3LNrFm0ndE3PQvOo7rg19QAKeOHGfxkI85l37GdFxs/n48PfcVbP5+WK0Wtv+wkWWTvqH7iIep064B9nM5JB9KZNZzH3MmzTN+xmMmjeT29s05mZzCva0fAaD/s3259+FupJxIAeD9N6awduUGkzHzk8l4ikfNxztyal88fiGBAJRrfhMV7mzA0rYjcJzLwf+6koYTnudIPELGawOcT5SFkDe/Inv7Oqw31sWvbnMyXu0POdmo0FJmg/5D/cc6cjIunhKhzp9x44F3c2jdbjZ9tITGA+6myYC7Wf3GXMMpnexZ2Xz973FkZ2ZhsVl5dP4o9kf/RocxvVn5+tcc3riHWx5oRdMnurB64nzTccnJyub9nmM5l5v3mflj2B0dw561O1n81mwcdgfdXuhJhwHdWTT+a9NxAVg893vmzPiG1z8YnW//rKlz+Pxjz8h4ES9qAVtMB3BVUFQ417etR9zX0Xn7buzVjt8nL8Fxzjn/Z9aJNEPpCmatWQ/H8QT0ySRK3H4XWcvnQo5zgm99+pThdOeFRIZTpW09dsyJzttXrX0Dfp+/BoDf56+hWoeGhtJdWnZmFgAWmxWLnw00hFeJ4vDGPQAcWLOLGp0amYyYz7ncvFabFavNhtaaPWt24LA7i8aB7fsIi7zOZMR8tv4aw6lUz/zv6rIc2vXNsCsuwEqpPu4MUpgGYx5h+2uz8/3QQqtGEtGkBh2XvkL7b0dyXd0q1zKSy/watSZ7czQAlnLlsVWvQ/AL7xE0bAKWG240G+4CbV55hNXj8v+Mg8qUJCMpFYCMpFSCynjOuwxwdpk8tux1hmz7iANrdhIfs5/jfxymevv6ANTs0oTQqHDDKc9TFsULy95k/NZP2bN2B3/FxOU73uz+O9gdvd1QOtc99FgP5q/6kjGTRhJaKtR0nPzsdtc3w66mBTzmcgeUUv2UUluUUltWZe67iks4lW9Xj7PJaZzceTDffovVQolSwSy/6xW2vTqblp8MuupruZ3Vhq1uU3K2rnY+t1ghKISM8UM4++00gvqNNJsvV5W29chMTiPxHz9jT6cdmhmdRzK56WCur1eVMjdW4PvnPqVBr/b8Z+mr+AcH4Mj2nBUStEMzvvNwXmrWnxvqViPqxop5x+4ceA8Ou53NC9caTFi4uZ99R5cmPbi/bS+SE5N59pXBpiPlox0OlzfTCuwDVkrtuNwhoNzlvk9rPRWYCjDr+keuup1fttGNVOhQn/Jt62L198MvNJAWH/QnMyGFw8u2AHAi5k+0Q+MfHkrWydNXe0m3sdVphONQHPp0KgA6NZmc7esAcBzcC9qBCimFTjfbFVG+4Y1UbV+fynfUxebvR4nQQDq/25/M5DSCI8LISEolOCKMzGTPfDualZbJoQ2xVGl9C5umLmPOo28CEF45kqpt6pkNdwln0jLZ9+tuat1el4Q/DtPkvlbUaVuf93u+ajpaoU4mp+Q9/varRUz+8m2DaS7BA7oWXFVYC7gc0Au4+xLbieKNdl7MG/NY0HAwC5sMZW3/Dzm2djfrnvqYw8u3UO62WgCEVonEUsLmUcUX8nc/AGTHrMdWox4AlojyYPUzXnwB1rw5j0+aDObTFkNZOuhDDq3fzbKnP2b/im3U7tESgNo9WhK3YqvhpOcFhofiXzIIcI4wqHRbHU7GxRP0981YpWj+VDe2f7XSYMrzQsJDCczN6+fvR40WdUjcH89Nt9el3ZPd+OTxt8g+e85wysKViTjfR92mU2v27fnTYJpL0A7XN8MKGwWxFAjRWsf884BSKro4AhXF/jm/0Oydfty16g0c2XbWD/nEdKT8/Pyx3lSfM7Pey9uVve5HAno/Q/DoT8CezZnPJhgMWLiNHy3h7o+f4uYHbyct/gRLnnzfdKQ8IRFh3PXOE1gsFpRFEbt0I3GrYmjY504a9GoHwN7lW9gxb7XhpE4lI0rz6MQBuXktbPt+A7tWbePl6PewlbAxaNZLABzcvo85I6cZTuv05sdjaNi8PmHhYazYtoiPJkyjYfNbqVnnRrTWxB9OYOxzb5qOmZ8XtYCVLuYxc+7ogriWvHNZ+rKmIxSZLEtf/LxxWfodxzZc9W9GxuiHXK45wWPnFHg9pdRQ4HFAAzuBPkAQMBeoBBwEHtBap1zmFAXymmFoQgjhEjd1QSilygODgYZa6zqAFXgIeAFYqbWuDqzMfX5FpAALIXyLe8cB24BApZQNZ8s3HugGfJ57/HOg+5VGlQIshPApRRmGduGQ2dytX955tD4KvA0cAhKAU1rrn4ByWuuE3NckABFXmtWrPooshBCFKsJNuAuHzP6TUqo0ztZuZSAV+EYp9YgbEuaRAiyE8C3uGwXRDjigtT4OoJT6DmgOJCqlorTWCUqpKCDpSi8gXRBCCN/ivo8iHwKaKqWClFIKaAvEAouB3rmv6Q0sutKo0gIWQvgUd60Jp7XeqJSaD2wDcoDtOLsrQoB5Sqm+OIv0/Vd6DSnAQgjf4sYPYmitXwZe/sfuLJyt4asmBVgI4Vs8YJIdV0kBFkL4Fi/6KLIUYCGEb5ECLIQQZmi7dEHkGZaxpbgv4VbfL69pOkKRBVvOmo5QZIkO78p8LNsz50EuyF/pVzw81btJC1gIIcxw1zC0a0EKsBDCt0gBFkIIQ7ynC1gKsBDCt+gc76nAUoCFEL7Fe+qvFGAhhG+Rm3BCCGGKtICFEMIMaQELIYQp0gIWQggzdI7pBK6TAiyE8CmFrDbvUaQACyF8ixRgIYQwQ1rAQghhiBTgYvbEgN707NUDtCZ29z6GDBhBVtY507HyCY8qw4BJQwgrG4Z2aFZ+/RPLZy5l8ORniapSHoDgksFkpGUwovNQw2nB5u/H8Lljsfn7YbFa2frDBhZPmkdwqRCemDyU6ypEcOJIElMGvkNmWobpuACUiSrD0EnPULpsabR2sPzrH1kyY3He8Xv63cNjL/Xl4bo9SUvxjOkkR70znNvaNSclOYWH2vwHgHFTXuGGqhUBCCkZQnpaOg+372sw5eWVKhXKBx++wU21bkRrzcD+L7B503bTsfLRdmU6gsu8rgBHRkXw+JOP0rJxF86ezWLqZ5Pofl8X5n69wHS0fBx2O7Nem8nBXX8SEBzAuKUT2bk2hvcHvZ33mkde6uMxxSwnK5u3e44hK/MsVpuV4fNfY1f0durf2YTY9Tv54eOFdOrfnU4D7uHb8bNMxwXAbrcz47Xp7N+1n8DgQCZ9/y4xa7ZzeN9hykSVoV7LW0k64llz4i6du5x5Mxcw5r0X8/a9+OQreY+fHj2Q9NPpBpK5Zvxbo/nfitX0emQQfn5+BAUFmI50EW9qAVsKe4FSqqZSqq1SKuQf+zsWX6yCWa1WAgIDsFqtBAUGcuyYZ/1HBpCalMLBXX8CcDbjLEfjjhBe7rp8r2napQXrF68xEe+SsjKdk6RbbVasNitaQ732jVg/PxqA9fOjubV9I4MJ80tJSmH/rv0AnMk4w+G4w1wX6fwZP/7yf5k5biZae9ag/O0bfyuwNd6u6x38uHDlNUzkutDQEFq0aMQXn88DIDs7m1OnThtOdTHtUC5vphVYgJVSg4FFwFPALqVUtwsOjyvOYJdzLCGJjz+YwbZdq9jxxxrS0k7zy6p1JqK4rEyFCCrVrkJczB95+2o2rsWp5FSOHUwwmCw/ZbEwetkE3tk6nd1rd3AgZh8ly4Zx6ngqAKeOpxJappTZkJcRUSGCqrWrsHf7Xhq3b8yJYyc4GHvAdKwiubVJXU4cP8nhA0dMR7mkSpUqkpx8ko+mvMWadYv5YPI4goICTce6iHa4vplWWAv4v0ADrXV3oDUwSik1JPfYZf98KKX6KaW2KKW2nDmX6o6ceUqFlaRjl7Y0uqUddWu0IigokPseuNut13An/6AAhk4Zzhdjp3Mm/Uze/uZdW3pU6xdAOxyM7fwczzV7gsp1q3H9jRVNR3JJQFAAIz55kU/HfIojx8EDgx7kq4me0U1SFB26t+UnD239AthsNurWq830aV/RskVXMjLPMHTYk6ZjXURr5fJmWmEF2Kq1TgfQWh/EWYQ7KaXeoYACrLWeqrVuqLVuGFgizE1RnVq1bsahv45w4kQKOTk5fL9kBY2a3OrWa7iL1WZl6JThrFv4C5uX/5q332K10LhjMzYsWWsw3eWdSctk76+/U+f2W0k7nkqpsmEAlCobxunkU2bD/YPVZmXEJy8SvSCaDcs3EHlDJOUqluP95R8wbd10ykSV4d1l7xKW+2/wVFarlTs6t2LF4lWmo1zW0aMJHD16jK1bfgNg0cIfqFu3tuFUF/OlFvAxpVS9v5/kFuO7gDLAzcWY67KOHk6gfsO6BAY6O/9b3t6MfXv/NBGlUP3eGkR83BGWTVucb//Nt9Ulfv8RTh47YSjZxULCSxJYMggAP/8S3NTiFo7tP0rM/7bQvEdrAJr3aE3Mis0GU15s8IQhHI47zKJpCwH4a+9fPFr/ER5v0ZfHW/QlOSGZpzs/TWpuN4qnatyyAX/FHSIp4bjpKJeVlJTM0aMJVKteGYDbWzdn7544w6ku5rArlzfTChsF0QvI98lqrXUO0Esp9UmxpSrAtq07WLroJ1as/g57Tg47d8Ty5WdzTUQpUI2GN9Hqvjs4FHuQN5ZNAmDuhFnE/LyVZnd7XvdDWERpHps4CIvFgrIoNn+/nh2rtrJ/216e/HAYtz3QlpPxyUwZMNF01Dy1GtWizX1tOBB7gPd+eB+AL976gq0/e+5K3K99NJoGzW4lLLwUS7fMZ+rEmSye/T0durXlx4X/Mx2vUM8PG8O06ZPwK+HHwQOHGdj/edORLuIJN9dcpYr7LnG5UjU96zZ0IdqEeeGy9MrrRhN637L0OZ4xjrgo/kg7ajpCkZ1K33/V1fNgvfYu15xKMSuMVmvv+y9XCCEK4GEjDwskBVgI4VO8qQtCCrAQwqd4wvAyVxX6STghhPAmdrtyeSuMUipMKTVfKbVHKRWrlGqmlApXSq1QSu3L/Vr6SrNKARZC+BQ3fxDjPWC51romUBeIBV4AVmqtqwMrc59fESnAQgif4q65IJRSJYFWwHQArfU5rXUq0A34PPdlnwPdrzSrFGAhhE/R2vXtwmkTcrd+F5yqCnAcmKmU2q6UmqaUCgbKaa0TnNfSCUDElWaVm3BCCJ9SlFEQWuupwNTLHLYB9YGntNYblVLvcRXdDZciLWAhhE+xOywub4U4AhzRWm/MfT4fZ0FOVEpFAeR+veL5cKUACyF8SlG6IAo+jz4GHFZK1cjd1RbYDSwGeufu641zyt4rIl0QQgif4nDvOOCngK+UUiWAP4E+OBuu85RSfYFDwP1XenIpwEIIn+LOD2JorWOAhpc41NYd55cCLITwKTIXxAW8bXaxKa09b42rwvgP9LxVCQpjrdLAdIQiOfvakMJf5GE2z440HcEIN3dBFCtpAQshfIoLoxs8hhRgIYRP8aIeCCnAQgjfIl0QQghhiDdNRykFWAjhUzxgsWOXSQEWQvgUjbSAhRDCiBzpghBCCDOkBSyEEIZIH7AQQhgiLWAhhDBEWsBCCGGIXVrAQghhRhFWJDLOKwpweFQZBkwaQljZMLRDs/Lrn1g+cymDJz9LVJXyAASXDCYjLYMRnYcaTutkiaxA0IBR559HRHH2u88499N3AJTodD+BDz1J2sB70OlpxZZj9KffsXr7XsJLBvPd+MFXfb7Fa7bx6aJfAPhvt9vp2rI+ACM+msfvB+KxWS3UqVqBUX264WezFnq+l8a9w+p1mwgvHcbCWVMuOv7nX4cZ9fo77P4jjsH9etOnZ4+r/jecO3eOEa9OZPfefYSVKsnbY0dQPqoce/7Yz6tvTyY9IxOL1UK/Xg/Rqd3tV329C6mI8gT2fj7vueW6SLJ++Ap73E4C7h8AfiXAbufs/I9xHNrn1mtfqWabJ2PPOIu2O9A5drbcOYLKwx+kbMeGaIcmO/kUuwd/xLnEFNNRAXBIC9i9HHY7s16bycFdfxIQHMC4pRPZuTaG9we9nfeaR17qQ2ZahsGU+TmOHSF99BPOJ8pC6Ltzyd661vk0vCy22g1wJCcWe45uLW/l3+2bMnLK/CJ9X9/XpzG2332UL1s6b9+p9EymLPiZ2WP7o5TioVEf0br+TZQMDqRz87qM6+9cGOCFj+axIHoLD7RrUuh1unduT8/7uvLiq29f8nipkqG8MPRJVq3eUKT8AEcTEhn5+kQ+m/xWvv3fLf2JkqEh/DBvBsv+F807H81g4qsjCAjwZ9yoZ7mhYnmSjp/ggb5P0aJJA0qGhhT52pejk46SOSF3aktlIXjMZ+Ts2EDAg0+R9eMc7LFbsd7UAP+ufTgz+UW3Xfdqbb93DNknz0/VeujDxRx4cy4AFR7vROVhPdj7/Kem4uXjTZPxFDpvm1KqsVKqUe7jWkqpZ5RSnYs/2nmpSSkc3PUnAGczznI07gjh5a7L95qmXVqwfvGaaxnLZbbat+I4Ho8+4Vy7L7DnAM7OnXpNZo5uULMyJYMD8+07nHiC/m99zkOjPuI/r37KgfjjLp1r/c59NK1TlVIhQZQMDqRpnaqs2/EHAC3r1UAphVKKOlUqkJjiWqu+Yb2bKVUy9LLHrysdxs031cBmu7itsOTHVTz0+BDu6z2QMW+9j91ud+maq9ZsoFvndgB0aN2SjVtj0FpT6V8VuKGi8x1VRNnrCC8dRkrqKZfOeSWsN9ZFJyegU44DGhXg/P9JBQajT50stuu6gz39TN5ja5A/2oNmQXcUYTOtwBawUuploBNgU0qtAJoA0cALSqlbtdavF3/E/MpUiKBS7SrExfyRt69m41qcSk7l2MGEax3HJX5N7iD711UA2G5thiMlGcfhP43lGTtjES/16coNkWXYEXeY1z9bzLQX+xb6fUknTxMZXirvebnwUiSdzD+BfXaOnaXrYhj+aBe3577Q/oOHWL7yF76cMhE/m41X357M0p9+plundoV+b9LxE0RGlAHAZrMSEhxE6qk0Soed/7ft3L2X7OwcKpaPKrZ/g1/9lmRvWw1A1oJPCXxyLP5dHwNlIfO954rtulei3tyRaA3xX64g/suVAFQZ8RCR97ci53Qm2+8dYzjheQ7lO10QPYB6gD9wDKigtU5TSk0ANgKXLMBKqX5AP4CG4XWpFlLJLWH9gwIYOmU4X4ydzpkL/gI379rSY1u/WG3Ybm3O2W+mQwl//O9+mIwJw43FyTybxW/7DvHcB3Py9p3LcbYcF67eytc/Ot/qH0o8yaC3v8DPZuX6sqV59+mH0Zd6c/eP3/Vxny+mQc1K1K9Rqbj+CQBs3BLD7j1xPNTX+XY+KyuL8NJhAAweMZaj8Ylk52STkHic+3oPBOCRB7pxT5cOl2ytqQv+oz2efJIRYyfw+kvDsFiKaXJvqw1r7SZkLfkCAL8WnclaMI2cHeux1buNgIcGc+bjUYWc5NrYetcoziWm4FemJPXmvUTmvnhSf43lzzfm8Ocbc7hhcHcqPNaRAxO+MR0VANfeB3mGwgpwjtbaDmQqpfZrrdMAtNZnlFKXbcFrracCUwH+fUN3t7w3sdqsDJ0ynHULf2Hz8l/z9lusFhp3bMaLdw1zx2XcznZLY+x/7UOnpWCpUBlL2UhCX50KOPuCQ8ZOIX3MQPSpa3MDw6E1oUEBzHt90EXHurdqQPdWzqWCLtUHXC68JJtjD+Q9Tzx5ikY3Vc57PuW7VaSkZTJqSLdi/Bc4aa3p2qkdQ/v3uejY+2+MBi7fB1wuogzHkpKJjChLTo6d9IzMvG6Q9IwMBjw3mqf69aZunZuKLb/tpgY4juxHp6cC4NeoDVnfOX8vcmLWEvDQU8V27aL6++ZadnIaycs2E3prNVJ/jc07nvjdWm756gWPKcDeNAqisD/v55RSQbmP8xbxUkqV4hp3ofR7axDxcUdYNm1xvv0331aX+P1HOHnsxLWM4zK/pm3yuh8cRw5w+qkenH72YU4/+zD65HHSRz95zYovQEhgAOXLluanjbsAZyHb+5drXTfNb67Ohp1xpGWcIS3jDBt2xtH85uoAfBe9hfU79zF+4APF12q8QNOG9VgRvZYTKakAnEo7Tfwx125q3nFbUxYt+x8AP0WvoUmDuiilyM7OZsiIV+nasS13tmlZXNEBsNVvRfa2X/KeO9JOYq1WBwBr9VtwHI8v1uu7yhLkjzU4IO9xeOtbyNhziMDK59ebK3NnQzL3eUZecI6CcHUzrbAWcCutdRaA1vrCgusH9C62VP9Qo+FNtLrvDg7FHuSNZZMAmDthFjE/b6XZ3R7c/VDCH1udBpz5bJKxCMM/nMuW2AOkpmfSfvBb9L+3DeP638/rny3h00XR5Njt3Nn0ZmrcUHhfZ6mQIPp1v4Oeoz8G4Il77qBUiPPv82szFxNVphS9xnwCQJuGtXjynjaFnvO5l8ezefsOUlPTaNv9EQb0fZScnBwAHrynC8knTvJg38HOoWEWC7PmLWTRV59QtfINPPXfXvR7eiQO7cDPZmPkMwO4PrJcode89647GfHqBDo98BilSoYyYcwLACxftYatMbtIPXWahbkF+vWRz1DzxqqFnrNI/Pyx1ajH2Xkf5u3KmjMZ/3v/CxYr5Jzj7NzJ7r3mFSpRthQ3z3wWAGW1krhgLSd//o0604cRVC0KHJqzR5LZ89xUw0nP85zbgYVTxX330l1dENeKrIp8bciqyMVv8+zAwl/kYdokzrvqZukX5R9xueb0OjrLaDPYK8YBCyGEqzxheJmrpAALIXyK3XzXrsukAAshfIq0gIUQwhApwEIIYYgXLQknBVgI4VukBSyEEIZ400eRi/8jS0IIcQ05lOubK5RSVqXUdqXU0tzn4UqpFUqpfblfSxd2jsuRAiyE8CnFMB3lECD2gucvACu11tWBlbnPr4gUYCGET3FnAVZKVQC6ANMu2N0N+Dz38edA9yvNKgVYCOFTdBE2pVQ/pdSWC7Z+/zjdu8Dz5K/X5bTWCQC5XyOuNKvchBNC+JSiTEd54dS5/6SUugtI0lpvVUq1dke2f5ICLITwKW4cBdEC6Jq7BFsAUFIpNQtIVEpFaa0TlFJRQNKVXqDYC/CS478V9yXc6rpfGpuOUGRVf15kOkKRBWrvyty9svfNkhdgKWE6ghEON01IqbUeAYwAyG0BP6u1fiR3RaDewPjcr1f8yywtYCGET7kGH8QYD8xTSvUFDgH3X+mJpAALIXxKcUxArrWOxrkgMVrrE0Bbd5xXCrAQwqfIR5GFEMKQHOU9i/BIARZC+BTvKb9SgIUQPka6IIQQwhB3DUO7FqQACyF8iveUXynAQggfI10QQghhiN2L2sBSgIUQPkVawEIIYYiWFrAQQpghLeBi9nvsGtJPp2N3OMjJyaHVbd1MR7qIzd+Pp+e+gs3fD6vVwvYfNrJs0jd0H/Ewddo1wH4uh+RDicx67mPOpGWajptHWRQ9l75KemIKi/pMpHqXxjQbei/h1a5ndteXSdxxwHTEiyiL4r7vXyXjWAo/9JlIo2d7UKlDfbRDc+ZEGj8/8wmZiammYwKgQkIo9fxz2CpXBjSnxr9J0P09sFX8FwCWkBAc6emc6Pu42aAXqL/pY+zpZ8DuQNvt7Og4nIrDHiDi4XbknEgD4K83viZ11TbDSZ1kGNo10LlTT06cSDEd47JysrJ5v+dYzmVmYbFZeWb+GHZHx7Bn7U4WvzUbh91Btxd60mFAdxaN/9p03Dy3PtaRk3HxlAgNBODE3iMs6fcebd94zHCyy7u5b0dS4uIpEeLMHDPleza/PR+AOn060GDIPax5cabJiHlKDh5E1sZNpI5+GWw2VEAAp14Zm3c8dGB/HOkZBhNe2u89XibnZP4pOROmLiV+ymJDiS7Pe8qvLElUrM5lZgFgtVmx2mxordmzZgcOu/NN0oHt+wiLvM5kxHxCIsOp3LYeu+ZE5+07GRdPyp8J5kIVIjgynH+1qUfs7Oi8fdnpZ/Ie+wX54yn/SaqgIPzq1uXM9987d+TkoNPT870m4I47OLtypYF0viMH7fJmWpFbwEqpL7TWvYojjKu01ixa8gVaa2ZMn83MGbNNxrksZVEMXzqesjdEsvrLH/krJi7f8Wb338G2pesNpbtY61ceYc242ZQIDjQdxWXNX3mEXy+RufHz93Pjfbdx7nQmix8YZyhdftbrr8eRmkqpES9gq1qV7D/+4PT7H6DPngXAr+4tOE6mYD9y1HDSf9CaWnNGg9YkfrmCxFkrAIh8rBNl729N+m9xHBzzOfZTntFy96abcAW2gJVSi/+xLQHu/ft5Ad+Xt9Bddo77VxJo17YHtzW/m3u796Ffv0dp0cIzV7HQDs34zsN5qVl/bqhbjagbK+Ydu3PgPTjsdjYvXGsw4XmV29YjMzmNpJ0HTUdx2b/a1uPsiTSSL5F501vfMKvJEPYtWE+d/7S/9uEuxWrFr/qNZC5cxInH/4s+e4bgh3vmHQ5s25YzHtj63dl1JDs6PEdsz9eI/E9HSjatxbHPf2Rb04H81m4Y2UmpVHq5t+mYeYphWfpiU1gXRAUgDXgHmJi7nb7g8SVpradqrRtqrRv62ULdlTXPsQTnEkzHj59gyZIfadCwrtuv4U5n0jLZ9+tuat3uzNnkvlbUaVufz4Z8YDjZedc3vJEq7evz2LpJdJ48kIrNa9Hx3f6mYxUosuGN3NC+Pg+vn0S7DwdyfYtatHkvf+Z9C9dTpXMjQwnzcxw/juP4cbJjYwE4G/0LthurOw9arfi3asnZVT8bTHhp2YnOey3ZJ9I4+cNGQupVIzv5FDgczlbxrBWE3lrdcMrzdBH+Z1phBbghsBUYCZzKnRX+jNb6F631L8Ud7lKCggIJCQnOe9ymbUt2795rIkqBQsJDCSwZBICfvx81WtQhcX88N91el3ZPduOTx98i++w5wynPW/fmPKY1GcyMFkNZNuhDDq/fzfKnPzYdq0Cb3pzHrMaD+ar5UP438EPi1+1m1ZCPKVWpXN5rKrWvT0qcZ/RhO06exJ6UhLWi852Qf4MG2A/+BUCJBg2wHzqE4/hxkxEvYgn0xxIckPe41O11ydx7CL+IsLzXhHduQuaeQ4YSXsybWsAF9gFrrR3AJKXUN7lfEwv7nuIWEVGG2XM+AcBmszJv3mL+t2K1yUiXVDKiNI9OHIDFYkFZLGz7fgO7Vm3j5ej3sJWwMWjWSwAc3L6POSOnGU57eVXvbMgdY3sRGB5Kt5nPcnz3Xyx49C3TsQrUZMSDhFWNQjs0p48ke8wICIC0994nbNRL4GfDHp/AqTfGAxDYtg1n/rfKcLqL+ZUNo+aM5wFQNivHF6wh9ecYqn0wmODalUBD1uEk9j8/xWzQC9i1+Zatq5QuQlilVBeghdb6RVe/JySosvf8NID/RHhmf3JBqjr8TEcoskCv+q2A7pWPmI5QZH/u85wRNq5qnvCtutpz9LzhHpd/u77+a8FVX+9qFKk1q7X+Hvi+mLIIIcRV84S+XVd57QcxhBDiUjyhb9dVUoCFED5FPooshBCGSBeEEEIY4k2jIKQACyF8inRBCCGEIXITTgghDJE+YCGEMES6IIQQwpCifLrXNCnAQgif4k3L0suKGEIIn+JAu7wVRClVUSn1s1IqVin1u1JqSO7+cKXUCqXUvtyvpa80qxRgIYRP0Vq7vBUiBximtb4JaAoMVErVAl4AVmqtqwMrc59fkWLvggj28y/uS7iVN719+dvcHO+bqauyX5jpCEVyixfOLPZTiQDTEYqsuRvO4a6bcFrrBCAh9/FppVQsUB7oBrTOfdnnQDQw/EquIS1gIYRPKcqKGBcun5a79bvUOZVSlYBbgY1Audzi/HeRjrjSrHITTgjhU4ryUWSt9VRgakGvUUqFAN8CT2ut05Ry3xTCUoCFED7FneOAlVJ+OIvvV1rr73J3JyqlorTWCUqpKCDpSs8vXRBCCJ/ixlEQCpgOxGqt37ng0GLg72WgewOLrjSrtICFED7FjR/EaAE8CuxUSsXk7nsRGA/MU0r1BQ4B91/pBaQACyF8ihtHQawFLtfh29Yd15ACLITwKTIZjxBCGGLX3jMhpRRgIYRPkcl4hBDCEJmOUgghDJE+YCGEMMQhXRBCCGGGtICFEMIQGQVRzJ4Y0JuevXqA1sTu3seQASPIyjpnOlY+Nn8/hs0dg83fhsVqZfsPv7J00jfU79yULk/fT2S18rzZ7UUO7fzTdNQ8I995nhbtmpGSnMrDbfoAUL12NYaPf4YSASWw59iZMGISu2P2GE7qFB5VhgGThhBWNgzt0Kz8+ieWz1zK4MnPElWlPADBJYPJSMtgROehhtOeV3/Tx9jTz4Ddgbbb2dFxOBWHPUDEw+3IOZEGwF9vfE3qqm2Gkzp/j/vMG4W1hA2LzcruZZuInvQtPSY/RZkqUQAElAzibFomUzq/aDitk3RBFKPIqAgef/JRWjbuwtmzWUz9bBLd7+vC3K8XmI6WT05WNu/2HENWZhYWm5Vn54/l9+gY4vceZuqTb9Nz3CVnvTPq+7nLmT9zAaPfO/8f0qCXnmD6O5+x4edNNGvThEEvPcmAHk+bC3kBh93OrNdmcnDXnwQEBzBu6UR2ro3h/UFv573mkZf6kJmWYTDlpf3e42VyTp7Oty9h6lLipyw2lOjScrKy+fzfr3Mu9/f4sfmjiYv+jfmDPsh7TYeXHiYrLdNgyvy8qQuiSJPxKKVuU0o9o5TqUFyBXGG1WgkIDMBqtRIUGMixY1c8GVGxysrMAsBqs2K1WdFac2z/URL/TDCc7NJiNu4gLSV/UdBaExwaDEBIyWCOJyabiHZJqUkpHNzlfAdxNuMsR+OOEF4u/8TpTbu0YP3iNSbi+YxzF/4e+1kvGmdbu0sTdi5ebyLaJTm0dnkzrcAWsFJqk9a6ce7j/wIDgQXAy0qp+lrr8dcgYz7HEpL4+IMZbNu1ijNns/hl1Tp+WbXuWsdwibIoRix9k7I3RPLLlz9yMCbOdKQie3f0ZN6dPYGnRvdHKUW/roNMR7qkMhUiqFS7CnExf+Ttq9m4FqeSUzl20MP+4GlNrTmjQWsSv1xB4qwVAEQ+1omy97cm/bc4Do75HPspz2i5K4viiaWvE16pHJu+WMHRmP15x25oXJOM5FOcPJhoMGF+vtQC9rvgcT+gvdZ6DNABePhy33ThLPNnzqVefcoLlAorSccubWl0Szvq1mhFUFAg9z1wt1uv4S7aoRnX+XlebPYklepW5fobK5qOVGT39u7Gey9/SLeGD/DeKx8y8p3nTUe6iH9QAEOnDOeLsdM5k34mb3/zri09svW7s+tIdnR4jtierxH5n46UbFqLY5//yLamA/mt3TCyk1Kp9HLvwk90jWiHZkrnF3mn6VOUr1eViBsr5B2r07UZOxdvMJjuYnZtd3kzrbACbFFKlVZKXQcorfVxAK11Bs4F6y5Jaz1Va91Qa90wsESY+9ICrVo349BfRzhxIoWcnBy+X7KCRk1udes13O1MWib7ft1NrdvrmY5SZJ3vv5Ofl60GYOWSaGrVq2k20D9YbVaGThnOuoW/sHn5r3n7LVYLjTs2Y8OStQbTXVp2Yorz64k0Tv6wkZB61chOPgUOh7NVPGsFobdWN5zyYmfTMjm4IZZqrW8BnD/jmzo24vclvxbyndeWGxflLHaFFeBSwFZgCxCulIqEvCU63LcuRxEcPZxA/YZ1CQx0LjjY8vZm7NvrOSMJ/hYSHkpgySAA/Pz9qNniZo7tP2o4VdElJ56gfrN6ADS8rT6HD3jWAqD93hpEfNwRlk3Lf/Pq5tvqEr//CCePnTCU7NIsgf5YggPyHpe6vS6Zew/hFxGW95rwzk3I3HPIUML8gsJDCcj9Pbb5+1Hlttokxzm7dKrcVofk/fGkHTtpMuJF3DUh+7VQYB+w1rrSZQ45gHvcnsYF27buYOmin1ix+jvsOTns3BHLl5/NNRGlQKUiStN74kCUxYLFotj6/QZ2rdpG3Tsb8eArjxESXpKBM17gSOxBPug1znRcAMZ+NIr6zeoRFl6KxVu+4dOJM3njubcZOnYQVquVc1nneOO5iaZj5qnR8CZa3XcHh2IP8saySQDMnTCLmJ+30uxuz+x+8CsbRs0Zzm4cZbNyfMEaUn+OodoHgwmuXQk0ZB1OYv/zU8wGzRUaEUb3d57EYrGgLIrfl27kj1XbAahzdzN2eVj3A3jXZDyquMOWK1XTe34awL2lbzEdoci2n/PMUSAF8bZl6Z86ZzUdoci8cVn6V/766qrfWUeF1XK55iSk7jbyTv5vXjcOWAghCuJNoyCkAAshfIp8FFkIIQzxpj5gKcBCCJ/iCZ9wc5UUYCGET5EWsBBCGOIJ43tdJQVYCOFTpAUshBCGyCgIIYQwRG7CCSGEIdIFIYQQhsgn4YQQwhBpAQshhCHe1Adc7LOhFSelVD+t9VTTOVzlbXnB+zJ7W16QzP+fFWlRTg/keUsLF8zb8oL3Zfa2vCCZ/9/y9gIshBBeSwqwEEIY4u0F2Nv6oLwtL3hfZm/LC5L5/y2vvgknhBDezNtbwEII4bWkAAshhCFeWYCVUh2VUnuVUnFKqRdM5ymMUmqGUipJKbXLdBZXKKUqKqV+VkrFKqV+V0oNMZ2pMEqpAKXUJqXUb7mZx5jO5AqllFUptV0ptdR0FlcopQ4qpXYqpWKUUltM5/F2XtcHrJSyAn8A7YEjwGbg31rr3UaDFUAp1QpIB77QWtcxnacwSqkoIEprvU0pFQpsBbp7+M9YAcFa63SllB+wFhiitf7VcLQCKaWeARoCJbXWd5nOUxil1EGgodY62XQWX+CNLeDGQJzW+k+t9TlgDtDNcKYCaa1XAydN53CV1jpBa70t9/FpIBYobzZVwbRTeu5Tv9zNo1sXSqkKQBdgmukswgxvLMDlgcMXPD+ChxcHb6aUqgTcCmw0HKVQuW/nY4AkYIXW2tMzvws8D3jPDOLOP2o/KaW2KqXk03BXyRsLsLrEPo9u6XgrpVQI8C3wtNY6zXSewmit7VrrekAFoLFSymO7e5RSdwFJWuutprMUUQutdX2gEzAwt3tNXCFvLMBHgIoXPK8AxBvK4rNy+1G/Bb7SWn9nOk9RaK1TgWigo9kkBWoBdM3tU50DtFFKzTIbqXBa6/jcr0nAApxdguIKeWMB3gxUV0pVVkqVAB4CFhvO5FNyb2hNB2K11u+YzuMKpVRZpVRY7uNAoB2wx2ioAmitR2itK2itK+H8HV6ltX7EcKwCKaWCc2/KopQKBjoAXjGyx1N5XQHWWucAg4Afcd4cmqe1/t1sqoIppWYDG4AaSqkjSqm+pjMVogXwKM5WWUzu1tl0qEJEAT8rpXbg/CO9QmvtFUO7vEg5YK1S6jdgE/C91nq54UxezeuGoQkhhK/wuhawEEL4CinAQghhiBRgIYQwRAqwEEIYIgVYCCEMkQIshBCGSAEWQghD/g85zDV27YG8nQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cf_matrix = confusion_matrix(np.argmax(outputs_valid[1].detach().cpu().numpy(), axis=1), \n",
    "                             bert_labels_valid.detach().numpy())\n",
    "print(cf_matrix)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(cf_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "julian-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('huggingface_bert_model6.data', 'wb') as filehandle:\n",
    "    pickle.dump(bert_training_stats, filehandle)\n",
    "    \n",
    "torch.save(bert_model.state_dict(), \"huggingface_bert6.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-mediterranean",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-7.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-7:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
