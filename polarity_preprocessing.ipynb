{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polarity_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIkPuGdDsYbl"
      },
      "source": [
        "!pip3 install transformers\n",
        "!cp /content/drive/MyDrive/fake-news-explainability/utils_fake_news.py .\n",
        "%run utils_fake_news.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd64mBu1_HI3"
      },
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Zk8CdYkx-uy"
      },
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/liar_valid.tsv', \n",
        "#                  delimiter='\\t',\n",
        "#                  header=None)\n",
        "# df.columns = ['ID','label','statement','subject','speaker',\n",
        "#               'job_title','state','party','barely_true_count',\n",
        "#               'false_count','half_true_count','mostly_true_count',\n",
        "#               'pants_on_fire_count','context']\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/fake_news.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra6JYNRaUEtz"
      },
      "source": [
        "BOOSTER_DICT = ['absolutely', 'amazingly', 'awfully', 'barely',\n",
        "                'completely', 'considerably', 'decidedly', 'deeply', \n",
        "                'enormously', 'entirely', 'especially', 'exceptionally',\n",
        "                'exclusively', 'extremely', 'fully', 'greatly', 'hardly',\n",
        "                'hella', 'highly', 'hugely', 'incredibly', 'intensely',\n",
        "                'majorly', 'overwhelmingly', 'really', 'remarkably',\n",
        "                'substantially', 'thoroughly', 'totally', 'tremendously',\n",
        "                'unbelievably', 'unusually', 'utterly', 'very']\n",
        "                \n",
        "df['polar_word'] = df['statement'].apply(lambda s: any(w in BOOSTER_DICT for w in s.split())) \n",
        "df = df.loc[df.polar_word].reset_index(drop=True)\n",
        "df['statement_new'] = df['statement'].apply(lambda s: ' '.join([w for w in s.split() if w.lower() not in BOOSTER_DICT]))\n",
        "df_orig = df[['id','label','statement']]\n",
        "df_new  = df[['id','label','statement_new']].rename(columns={'statement_new':'statement'})\n",
        "\n",
        "liar_encode = {'barely-true':1, 'false':1, 'pants-fire':1, \n",
        "               'half-true':0, 'mostly-true':0, 'true':0}\n",
        "\n",
        "# df_orig['label'] = df_orig['label'].apply(lambda x: liar_encode[x])\n",
        "# df_new['label']  = df_new['label'].apply(lambda x: liar_encode[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI7pWFZzG2Dh"
      },
      "source": [
        "df_orig.to_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/fake_news_polarity_orig.csv', index=False)\n",
        "df_new.to_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/fake_news_polarity_new.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-osKGPwhDLkI",
        "outputId": "8129dcc4-d28f-4457-d86a-c5418f39ea54"
      },
      "source": [
        "torch.save(encode_dataframe(df_orig['statement'], df_orig['label']),\n",
        "           '/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_polarity_orig.pt')\n",
        "torch.save(encode_dataframe(df_new['statement'], df_new['label']),\n",
        "           '/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_polarity_new.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2132: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QmvfGQ8wJDa"
      },
      "source": [
        "df_orig = pd.read_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/fake_news_polarity_orig.csv')\n",
        "df_new  = pd.read_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/fake_news_polarity_new.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kajQBRlQEcnY"
      },
      "source": [
        "df_orig = df_orig.loc[df_orig.label==1].reset_index(drop=True)\n",
        "df_new = df_new.loc[df_new.label==1].reset_index(drop=True)\n",
        "\n",
        "df_orig.to_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/fake_news_polarity_orig_filtered.csv')\n",
        "df_new.to_csv('/content/drive/MyDrive/fake-news-explainability/Data/Raw/fake_news_polarity_new_filtered.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMbVxSiarLiH",
        "outputId": "d0a9c58d-41ea-4761-fd97-3ae94c611aab"
      },
      "source": [
        "torch.save(encode_dataframe(df_orig['statement'], df_orig['label']),\n",
        "           '/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_polarity_orig_filtered.pt')\n",
        "torch.save(encode_dataframe(df_new['statement'], df_new['label']),\n",
        "           '/content/drive/MyDrive/fake-news-explainability/Data/Encoded/fake_news/evaluation/fake_news_polarity_new_filtered.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2204: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzI2U3SCsHXA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}